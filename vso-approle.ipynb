{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HashiCorp Vault Demo for Vault Secrets Operator - Using Vault to sync secrets to K8s as the last mile provider to Applications\n",
    "\n",
    "This demo shows how HashiCorp Vault Secrets Operator works. This will be using the dynamic secrets engine for PostgreSQL as an example.  The PostgreSQL generated shortlived credentials will be synced to K8s secrets using the Vault Secrets Operator.\n",
    "\n",
    "The diagram below is a visual representation of this flow.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"images/vault-demo-vault-secrets-operator.png\">\n",
    "\n",
    "In this demo, the following Custom Resource Definitions (CRDs) are used:\n",
    "- VaultConnection (for connecting to Vault)\n",
    "- VaultAuth (for authenticating to Vault)\n",
    "- VaultDynamicSecret (for synchronizing with dynamic secrets)\n",
    "\n",
    "As part of VaultAuth, we will also be showing how the Vault transit engine can be used to encrypt the client storage cache within Kubernetes objects.\n",
    "\n",
    "Ref: https://developer.hashicorp.com/vault/docs/platform/k8s/vso/api-reference#storageencryption\n",
    "\n",
    "There are also other CRDs that provide other functionality such as:\n",
    "- VaultStaticSecret (for synchronizing with k2-v1 and k2-v2 static secrets)\n",
    "- VaultPKISecret (for synchronizing with PKI secrets)\n",
    "\n",
    "Ref: https://developer.hashicorp.com/vault/docs/platform/k8s/vso\n",
    "\n",
    "\n",
    "\n",
    "## Setup of the Demo\n",
    "\n",
    "This setup is tested on MacOS and is meant to simulate a distributed setup.  The components used in this demo are:\n",
    "- Vault Enterprise installed on docker (to simulate an external Vault)\n",
    "- PostgreSQL installed on docker (to simulate an external PostgreSQL database for the Dynamic secrets)\n",
    "- Minikube (to simulate a K8s cluster.  Vault will be syncing the dynamic secret to K8s secrets for the application pods to use.)\n",
    "\n",
    "Note that we will be using the docker and minikube routing to the host machine for communication between the docker Vault pod and the Kubernetes cluster.  The host machine will function as a network bridge for communication.\n",
    "\n",
    "## Requirements to Run This Demo\n",
    "You will need Visual Studio Code to be installed with the Jupyter plugin.  To run this notebook in VS Code, chose the Jupyter kernel and then Bash.\n",
    "- To run the current cell, use Ctrl + Enter.\n",
    "- To run the current cell and advance to the next, use Shift+Enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Pre-requisites (One-time)\n",
    "\n",
    "Assumes you have docker installed and brew installed\n",
    "\n",
    "- https://docs.docker.com/desktop/install/mac-install/\n",
    "- https://brew.sh/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Install minikube\n",
    "brew install minikube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install Kubectl CLI\n",
    "brew install kubernetes-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install Helm CLI.  This is used to install the VSO helm chart.\n",
    "brew install helm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install K9s.  This is a nice console GUI for K8s.  https://k9scli.io/\n",
    "brew install K9s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up HashiCorp Vault, PostgreSQL servers and K8s cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:41:07.970289Z",
     "start_time": "2024-10-11T02:41:06.559149Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01103e057497a80e916477d6dfbe8971897f458af2bd68d084d9c57f49bd78ff\n"
     ]
    }
   ],
   "source": [
    "# For this demo, we will be simulating a Vault server that is hosted external from the K8s cluster.  i.e. in Docker.\n",
    "export VAULT_PORT=8200\n",
    "export VAULT_ADDR=\"http://127.0.0.1:${VAULT_PORT}\"\n",
    "export VAULT_TOKEN=\"root\"\n",
    "\n",
    "# Change the path to your license file\n",
    "export VAULT_LICENSE=$(cat ../vault.hclic)\n",
    "\n",
    "# Refresh Vault docker image with latest version\n",
    "#docker pull hashicorp/vault-enterprise\n",
    "\n",
    "# Run Vault in docker in Dev mode with Enterprise license.\n",
    "# We have set VAULT_LOG_LEVEL to trace for troubleshooting purposes.  This will allow you to view detailed information as you test.\n",
    "docker run -d --rm --name vault-enterprise --cap-add=IPC_LOCK \\\n",
    "-e \"VAULT_DEV_ROOT_TOKEN_ID=${VAULT_TOKEN}\" \\\n",
    "-e \"VAULT_DEV_LISTEN_ADDRESS=:${VAULT_PORT}\" \\\n",
    "-e \"VAULT_LICENSE=${VAULT_LICENSE}\" \\\n",
    "-e \"VAULT_LOG_LEVEL=trace\" \\\n",
    "-p ${VAULT_PORT}:${VAULT_PORT} hashicorp/vault-enterprise:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:41:18.350251Z",
     "start_time": "2024-10-11T02:41:16.938021Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "What's next:\u001b[0m\n",
      "    Try Docker Debug for seamless, persistent debugging tools in any container or image → \u001b[36mdocker debug vault-enterprise\u001b[0m\n",
      "    Learn more at https://docs.docker.com/go/debug-cli/\n",
      "\u001b[1m\n",
      "What's next:\u001b[0m\n",
      "    Try Docker Debug for seamless, persistent debugging tools in any container or image → \u001b[36mdocker debug vault-enterprise\u001b[0m\n",
      "    Learn more at https://docs.docker.com/go/debug-cli/\n",
      "\u001b[1m\n",
      "What's next:\u001b[0m\n",
      "    Try Docker Debug for seamless, persistent debugging tools in any container or image → \u001b[36mdocker debug vault-enterprise\u001b[0m\n",
      "    Learn more at https://docs.docker.com/go/debug-cli/\n",
      "\u001b[0mSuccess! Enabled the file audit device at: file/\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Optional: You can enable file audit device for more information\n",
    "docker exec -it vault-enterprise /bin/sh -c \"mkdir /var/log/vault.d\"\n",
    "docker exec -it vault-enterprise /bin/sh -c \"touch /var/log/vault.d/vault_audit.log\"\n",
    "docker exec -it vault-enterprise /bin/sh -c \"chown -R vault:vault /var/log/vault.d\"\n",
    "vault audit enable file file_path=/var/log/vault.d/vault_audit.log\n",
    "\n",
    "# You can run the following command in the container terminal to follow the logs\n",
    "# tail -f /var/log/vault.d/vault_audit.log\n",
    "# Or you can run it from outside on your host machine\n",
    "# docker exec -it vault-enterprise /bin/sh -c \"tail -f /var/log/vault.d/vault_audit.log\"\n",
    "# Use Ctrl + C to break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:41:48.894081Z",
     "start_time": "2024-10-11T02:41:31.373996Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* minikube v1.35.0 on Darwin 14.7.1 (arm64)\n",
      "* Automatically selected the docker driver\n",
      "* Using Docker Desktop driver with root privileges\n",
      "* Starting \"minikube\" primary control-plane node in \"minikube\" cluster\n",
      "* Pulling base image v0.0.46 ...\n",
      "* Creating docker container (CPUs=2, Memory=7788MB) ...\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\n",
      "* Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\n",
      "  - Generating certificates and keys ...\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\n",
      "  - Booting up control plane ...\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\n",
      "  - Configuring RBAC rules ...\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\n",
      "* Configuring bridge CNI (Container Networking Interface) ...\n",
      "* Verifying Kubernetes components...\n",
      "  - Using image gcr.io/k8s-minikube/storage-provisioner:v5\n",
      "* Enabled addons: storage-provisioner, default-storageclass\n",
      "* Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n"
     ]
    }
   ],
   "source": [
    "# Start minikube\n",
    "minikube start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:41:52.388973Z",
     "start_time": "2024-10-11T02:41:51.551500Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5d0414dd9e24ddc4a1eb3b42bea92b87f5adaaad83b5d7eca37efc63f30fef7e\n"
     ]
    }
   ],
   "source": [
    "# Run a PostgreSQL database for the Dynamic Secret engine\n",
    "export PG_ADMIN_NAME=root\n",
    "export PG_ADMIN_PASSWORD=mypassword\n",
    "export PG_PORT=5432\n",
    "\n",
    "docker run --name postgres \\\n",
    "     -p $PG_PORT:$PG_PORT \\\n",
    "     --rm \\\n",
    "     -e POSTGRES_USER=$PG_ADMIN_NAME \\\n",
    "     -e POSTGRES_PASSWORD=$PG_ADMIN_PASSWORD \\\n",
    "     -d postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:41:54.103338Z",
     "start_time": "2024-10-11T02:41:53.872237Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                                 COMMAND                  CREATED          STATUS         PORTS                                                                                                                                  NAMES\n",
      "5d0414dd9e24   postgres                              \"docker-entrypoint.s…\"   2 seconds ago    Up 1 second    0.0.0.0:5432->5432/tcp                                                                                                                 postgres\n",
      "01103e057497   hashicorp/vault-enterprise:latest     \"docker-entrypoint.s…\"   5 minutes ago    Up 5 minutes   0.0.0.0:8200->8200/tcp                                                                                                                 vault-enterprise\n",
      "91ded15543c3   gcr.io/k8s-minikube/kicbase:v0.0.46   \"/usr/local/bin/entr…\"   17 minutes ago   Up 4 minutes   127.0.0.1:50289->22/tcp, 127.0.0.1:50290->2376/tcp, 127.0.0.1:50292->5000/tcp, 127.0.0.1:50293->8443/tcp, 127.0.0.1:50291->32443/tcp   minikube\n"
     ]
    }
   ],
   "source": [
    "# Verify Vault, minikube, and PostgreSQL containers are running\n",
    "docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Optional step for slow internet environments.  Pre-load the nginx:latest image in minikube before the demo.\n",
    "minikube image load nginx:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure PostgreSQL Database for Dynamic Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:41:58.743395Z",
     "start_time": "2024-10-11T02:41:58.579465Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Setup psql alias to the container to make it easier to do CLI commands to the postgresql pod\n",
    "alias psql=\"docker exec -it postgres psql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:42:02.951402Z",
     "start_time": "2024-10-11T02:42:01.789034Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE ROLE\n",
      "\u001b[1m\n",
      "What's next:\u001b[0m\n",
      "    Try Docker Debug for seamless, persistent debugging tools in any container or image → \u001b[36mdocker debug postgres\u001b[0m\n",
      "    Learn more at https://docs.docker.com/go/debug-cli/\n",
      "GRANT\n",
      "\u001b[1m\n",
      "What's next:\u001b[0m\n",
      "    Try Docker Debug for seamless, persistent debugging tools in any container or image → \u001b[36mdocker debug postgres\u001b[0m\n",
      "    Learn more at https://docs.docker.com/go/debug-cli/\n",
      "Expanded display is used automatically.\n",
      "                             List of roles\n",
      " Role name |                         Attributes                         \n",
      "-----------+------------------------------------------------------------\n",
      " readonly  | No inheritance, Cannot login\n",
      " root      | Superuser, Create role, Create DB, Replication, Bypass RLS\n",
      "\n",
      "\u001b[1m\n",
      "What's next:\u001b[0m\n",
      "    Try Docker Debug for seamless, persistent debugging tools in any container or image → \u001b[36mdocker debug postgres\u001b[0m\n",
      "    Learn more at https://docs.docker.com/go/debug-cli/\n"
     ]
    }
   ],
   "source": [
    "# Configure the name of the database role used for the Vault database engine\n",
    "export PGROLE=readonly\n",
    "\n",
    "# Drop database role if it exists\n",
    "#psql -U $PG_ADMIN_NAME -c \"DROP ROLE \\\"$PGROLE\\\";\"\n",
    "\n",
    "# Create a database role for Vault database engine to use\n",
    "psql -U $PG_ADMIN_NAME -c \"CREATE ROLE \\\"$PGROLE\\\" NOINHERIT;\"\n",
    "\n",
    "# Grant the ability to read all tables to the role\n",
    "psql -U $PG_ADMIN_NAME -c \"GRANT SELECT ON ALL TABLES IN SCHEMA public TO \\\"$PGROLE\\\";\"\n",
    "\n",
    "# list users and roles and verify that the role is created\n",
    "psql -U $PG_ADMIN_NAME -c '\\x auto;' -c \"\\dg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enable the Database Secrets Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mError enabling approle auth: Error making API request.\n",
      "\n",
      "URL: POST http://127.0.0.1:8200/v1/sys/auth/approle\n",
      "Code: 400. Errors:\n",
      "\n",
      "* path is already in use at approle/\u001b[0m\n",
      "\u001b[0mSuccess! Uploaded policy: agent-policy\u001b[0m\n",
      "\u001b[0mSuccess! Data written to: auth/approle/role/agent-app-role\u001b[0m\n",
      "\u001b[0mKey                        Value\n",
      "---                        -----\n",
      "bind_secret_id             true\n",
      "local_secret_ids           false\n",
      "secret_id_bound_cidrs      <nil>\n",
      "secret_id_num_uses         1\n",
      "secret_id_ttl              1m\n",
      "token_bound_cidrs          []\n",
      "token_explicit_max_ttl     0s\n",
      "token_max_ttl              1m\n",
      "token_no_default_policy    false\n",
      "token_num_uses             0\n",
      "token_period               0s\n",
      "token_policies             [agent-policy default]\n",
      "token_ttl                  30s\n",
      "token_type                 batch\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# AppRole\n",
    "\n",
    "# Enable approle Auth\n",
    "vault auth enable approle\n",
    "\n",
    "vault policy write agent-policy - << EOF\n",
    "path \"secret/data/demo-secrets\" {\n",
    "  capabilities = [\"read\"]\n",
    "}\n",
    "path \"auth/token/*\" {\n",
    "  capabilities = [\"create\", \"update\"]\n",
    "}\n",
    "path \"auth/approle/role/agent-app-role/secret-id\" {\n",
    "  capabilities = [\"read\", \"create\", \"update\"]\n",
    "}\n",
    "EOF\n",
    "\n",
    "# Create the approle for the agent with the above policy\n",
    "vault write auth/approle/role/agent-app-role \\\n",
    "    secret_id_ttl=1m \\\n",
    "    token_num_uses=0 \\\n",
    "    token_ttl=30s \\\n",
    "    token_max_ttl=60s \\\n",
    "    secret_id_num_uses=1 \\\n",
    "    token_type='batch' \\\n",
    "    token_policies=agent-policy,default\n",
    "\n",
    "# Show settings on app role\n",
    "vault read auth/approle/role/agent-app-role\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/secretkv-approle created\n"
     ]
    }
   ],
   "source": [
    "# Store roleid value for Vault Agent\n",
    "vault read -field=role_id auth/approle/role/agent-app-role/role-id > roleid\n",
    "\n",
    "vault write -force -field=secret_id auth/approle/role/agent-app-role/secret-id > secretid\n",
    "\n",
    "kubectl create secret generic secretkv-approle --from-file=id=secretid -n $KUBENAMESPACE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mKey                     Value\n",
      "---                     -----\n",
      "token                   hvb.AAAAAQKVE3A4quxbRcGMt4ymVFXQBIkHbiTZps2bQYUyzTaHdjlVJeZoYtvP13Knihh_MgGGTB83JS7Nqa6p7FZG8v-4yhNsdgCfIRCKkWPYY2D--h3DgwXjCRpa8kmXgsMdnrCZg2pmWijfPTv298jsP5bDY24JEpfb35tfWnWP8t0tRcVw4t2msw7jA2BqCDFYiiJcB0qvrtSZTvkrWD8AxULKVY9zXmclpkkI\n",
      "token_accessor          n/a\n",
      "token_duration          30s\n",
      "token_renewable         false\n",
      "token_policies          [\"agent-policy\" \"default\"]\n",
      "identity_policies       []\n",
      "policies                [\"agent-policy\" \"default\"]\n",
      "token_meta_role_name    agent-app-role\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "vault write auth/approle/login role_id=$(cat roleid) secret_id=$(cat secretid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mSuccess! Data written to: sys/locked-users/auth_approle_db2e47e5/unlock/d9d6b340-1aca-ff73-911a-c54ec42d9d42\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "vault write -force /sys/locked-users/auth_approle_db2e47e5/unlock/d9d6b340-1aca-ff73-911a-c54ec42d9d42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mKey             Value\n",
      "---             -----\n",
      "by_namespace    []\n",
      "total           0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "vault read sys/locked-users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:42:32.550710Z",
     "start_time": "2024-10-11T02:42:31.314619Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mSuccess! Disabled the secrets engine (if it existed) at: database/\u001b[0m\n",
      "\u001b[0mSuccess! Enabled the database secrets engine at: database/\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Set the name of the PostgreSQL Database Secret path\n",
    "export DBPATH=database\n",
    "\n",
    "# Disable the database engine if it is there\n",
    "vault secrets disable $DBPATH\n",
    "\n",
    "# Enable the database secrets engine at the \"database/\" path\n",
    "vault secrets enable -path $DBPATH database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:42:35.104970Z",
     "start_time": "2024-10-11T02:42:34.018447Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postgres IP Address is: 172.17.0.3\n",
      "Database Engine Path is: database\n",
      "Postgres Role is: readonly\n",
      "Postgres Admin Username is: root\n",
      "Postgres Admin Password is: mypassword\n",
      "\u001b[0mSuccess! Data written to: database/config/postgresql\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# As both Vault and PostgreSQL is running on docker, Vault will be connecting to PostgreSQL via the docker bridge network\n",
    "# Obtain IP address of the postgres database for configuration\n",
    "export POSTGRES_DB_IP=$(docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' postgres)\n",
    "echo \"Postgres IP Address is: $POSTGRES_DB_IP\"\n",
    "echo \"Database Engine Path is: $DBPATH\"\n",
    "echo \"Postgres Role is: $PGROLE\"\n",
    "echo \"Postgres Admin Username is: $PG_ADMIN_NAME\"\n",
    "echo \"Postgres Admin Password is: $PG_ADMIN_PASSWORD\"\n",
    "\n",
    "# Configure the database secrets engine with the connection credentials for the PostgreSQL database.\n",
    "vault write $DBPATH/config/postgresql \\\n",
    "     plugin_name=postgresql-database-plugin \\\n",
    "     connection_url=\"postgresql://{{username}}:{{password}}@$POSTGRES_DB_IP/postgres?sslmode=disable\" \\\n",
    "     allowed_roles=$PGROLE \\\n",
    "     username=\"$PG_ADMIN_NAME\" \\\n",
    "     password=\"$PG_ADMIN_PASSWORD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:42:52.717678Z",
     "start_time": "2024-10-11T02:42:51.295022Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE ROLE \"{{name}}\" WITH LOGIN PASSWORD '{{password}}' VALID UNTIL '{{expiration}}' INHERIT;\n",
      "GRANT readonly TO \"{{name}}\";\n",
      "Database Engine Path is: database\n",
      "Postgres Role is: readonly\n",
      "\u001b[0mSuccess! Data written to: database/roles/readonly\u001b[0m\n",
      "\u001b[0mKey                      Value\n",
      "---                      -----\n",
      "creation_statements      [CREATE ROLE \"{{name}}\" WITH LOGIN PASSWORD '{{password}}' VALID UNTIL '{{expiration}}' INHERIT;\n",
      "GRANT readonly TO \"{{name}}\";]\n",
      "credential_type          password\n",
      "db_name                  postgresql\n",
      "default_ttl              45s\n",
      "max_ttl                  1m30s\n",
      "renew_statements         []\n",
      "revocation_statements    []\n",
      "rollback_statements      []\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Define the SQL used to create the database role.\n",
    "tee readonly.sql <<EOF\n",
    "CREATE ROLE \"{{name}}\" WITH LOGIN PASSWORD '{{password}}' VALID UNTIL '{{expiration}}' INHERIT;\n",
    "GRANT readonly TO \"{{name}}\";\n",
    "EOF\n",
    "\n",
    "# Create the dyanamic database role that creates credentials with the readonly.sql.\n",
    "# Using the same role name as the PostgreSQL role.  You can use a different name if needed.\n",
    "# Set TTL of credential to 45 secs and the max TTL to 90 secs for this demo\n",
    "echo \"Database Engine Path is: $DBPATH\"\n",
    "echo \"Postgres Role is: $PGROLE\"\n",
    "vault write $DBPATH/roles/$PGROLE \\\n",
    "      db_name=postgresql \\\n",
    "      creation_statements=@readonly.sql \\\n",
    "      default_ttl=45 \\\n",
    "      max_ttl=90\n",
    "\n",
    "# Remove the readonly.sql file\n",
    "rm readonly.sql\n",
    "\n",
    "# Show role settings\n",
    "vault read $DBPATH/roles/$PGROLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:44:05.307161Z",
     "start_time": "2024-10-11T02:44:04.901263Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Engine Path is: database\n",
      "Postgres Role is: readonly\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[1;34m\"request_id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"ecb9fa6e-44fc-58fa-191e-98a8dc4574c3\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"lease_id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"database/creds/readonly/NdJKlS9OMYDtNrFdrOq1TudH\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"lease_duration\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m45\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"renewable\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39mtrue\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
      "    \u001b[0m\u001b[1;34m\"password\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"0IiL1vEdi-FTUc40401l\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[1;34m\"username\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"v-token-readonly-m18571da1ohntXI6Zmvn-1742280522\"\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"warnings\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Read credentials from the readonly database role\n",
    "echo \"Database Engine Path is: $DBPATH\"\n",
    "echo \"Postgres Role is: $PGROLE\"\n",
    "results=$(vault read -format=json $DBPATH/creds/$PGROLE)\n",
    "echo $results | jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:44:07.445274Z",
     "start_time": "2024-10-11T02:44:06.655341Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic PostgreSQL username: v-token-readonly-m18571da1ohntXI6Zmvn-1742280522\n",
      "Dynamic PostgreSQL password: 0IiL1vEdi-FTUc40401l\n",
      "You are connected to database \"postgres\" as user \"v-token-readonly-m18571da1ohntXI6Zmvn-1742280522\" on host \"127.0.0.1\" at port \"5432\".\n",
      "\u001b[1m\n",
      "What's next:\u001b[0m\n",
      "    Try Docker Debug for seamless, persistent debugging tools in any container or image → \u001b[36mdocker debug postgres\u001b[0m\n",
      "    Learn more at https://docs.docker.com/go/debug-cli/\n"
     ]
    }
   ],
   "source": [
    "# Obtain dynamic postgres username and password\n",
    "export PGPASSWORD=$(echo $results | jq .data.password -r)\n",
    "export PGUSER=$(echo $results | jq .data.username -r)\n",
    "echo \"Dynamic PostgreSQL username: $PGUSER\"\n",
    "echo \"Dynamic PostgreSQL password: $PGPASSWORD\"\n",
    "\n",
    "# Connect to the postgres database using the dynamic credentials and show the connection information\n",
    "# Re-run after 30s to show that the credentials has expired\n",
    "psql \"postgresql://$PGUSER:$PGPASSWORD@127.0.0.1/postgres\" -c \"\\conninfo\"\n",
    "\n",
    "# You can also open the Docker dashboard and show the logs in the vault-enterprise container.  You should see the expiration of the credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Vault Secrets Operator\n",
    "\n",
    "## Configure Kubernetes namespace, service account, and token first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:44:31.316818Z",
     "start_time": "2024-10-11T02:44:30.397111Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Setup environment variables for K8s configuration\n",
    "\n",
    "# Specify the namespace of the secrets and demo apps\n",
    "export KUBENAMESPACE=demo-ns\n",
    "\n",
    "# Name of K8s service account used to authenticate to Vault for the dynamic database secret.\n",
    "export KUBESVCACCOUNT=vault-svc-account\n",
    "\n",
    "# Name of the K8s service account token used for verification when Vault connects to minikube for K8s JWT auth\n",
    "# This is required as Vault is external from the K8s\n",
    "export KUBESVCACCOUNTTOKEN=vault-token-demo\n",
    "\n",
    "# Since Vault is running on the host in docker.  From minikube, we will use the host DNS entry to connect back to the host to access Vault.\n",
    "# Note that this is currently fine as we do not have TLS configured on Vault. \n",
    "# If TLS is on, you need a proper DNS name and certificate configured.\n",
    "export MINIKUBEVAULTADDRESS=http://host.minikube.internal:8200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:44:43.311451Z",
     "start_time": "2024-10-11T02:44:42.794105Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating K8s namespace: demo-ns\n",
      "namespace/demo-ns created\n"
     ]
    }
   ],
   "source": [
    "# Delete namespace if it exists\n",
    "#kubectl delete ns $KUBENAMESPACE\n",
    "\n",
    "# Create a new K8s namespace for this demo\n",
    "echo \"Creating K8s namespace: $KUBENAMESPACE\"\n",
    "kubectl create ns $KUBENAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:44:45.825717Z",
     "start_time": "2024-10-11T02:44:45.108721Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K8s namespace: demo-ns\n",
      "K8s Service Account: vault-svc-account\n",
      "serviceaccount/vault-svc-account created\n"
     ]
    }
   ],
   "source": [
    "# Create a K8s service account for Vault to use\n",
    "echo \"K8s namespace: $KUBENAMESPACE\"\n",
    "echo \"K8s Service Account: $KUBESVCACCOUNT\"\n",
    "kubectl create -f - <<EOF\n",
    "apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: $KUBESVCACCOUNT\n",
    "  namespace: $KUBENAMESPACE\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:45:01.764562Z",
     "start_time": "2024-10-11T02:45:00.904921Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K8s namespace: demo-ns\n",
      "K8s Service Account Token: vault-token-demo\n",
      "secret/vault-token-demo created\n"
     ]
    }
   ],
   "source": [
    "# Create a service token for Vault to use in the K8s auth method and store it as a K8s secret\n",
    "echo \"K8s namespace: $KUBENAMESPACE\"\n",
    "echo \"K8s Service Account Token: $KUBESVCACCOUNTTOKEN\"\n",
    "kubectl create -f - <<EOF\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: $KUBESVCACCOUNTTOKEN\n",
    "  namespace: $KUBENAMESPACE\n",
    "  annotations:\n",
    "    kubernetes.io/service-account.name: $KUBESVCACCOUNT\n",
    "type: kubernetes.io/service-account-token\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:45:06.536859Z",
     "start_time": "2024-10-11T02:45:05.431725Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusterrolebinding.rbac.authorization.k8s.io/role-tokenreview-binding created\n"
     ]
    }
   ],
   "source": [
    "# As the Vault Server is running externally, create a clusterRoleBinding for\n",
    "# the namespace and service account used with the \"system:auth-delegator\" ClusterRole\n",
    "# This step is important otherwise the K8s authentication configured later will fail.\n",
    "kubectl create -f - <<EOF\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: ClusterRoleBinding\n",
    "metadata:\n",
    "  name: role-tokenreview-binding\n",
    "  namespace: $KUBENAMESPACE\n",
    "roleRef:\n",
    "  apiGroup: rbac.authorization.k8s.io\n",
    "  kind: ClusterRole\n",
    "  name: system:auth-delegator\n",
    "subjects:\n",
    "  - kind: ServiceAccount\n",
    "    name: $KUBESVCACCOUNT\n",
    "    namespace: $KUBENAMESPACE\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Vault Kubernetes Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:45:38.096324Z",
     "start_time": "2024-10-11T02:45:37.447993Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mSuccess! Disabled the auth method (if it existed) at: kubernetes-minikube/\u001b[0m\n",
      "\u001b[0mSuccess! Enabled kubernetes auth method at: kubernetes-minikube/\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Set the K8s auth path for the minikube cluster\n",
    "export K8SAUTHPATH=kubernetes-minikube\n",
    "# Role name to be used by minikube to read dynamic secrets\n",
    "export K8SROLE=auth-role-minikube\n",
    "\n",
    "# Disable K8s auth method if it exists\n",
    "vault auth disable $K8SAUTHPATH\n",
    "# Enable K8s auth method\n",
    "vault auth enable -path $K8SAUTHPATH kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:45:49.198623Z",
     "start_time": "2024-10-11T02:45:48.307876Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker Host IP address is: 192.168.65.254\n",
      "Updating Vault's /etc/hosts file to add IP of host.docker.internal and mapping it to kubernetes.default\n",
      "\u001b[1m\n",
      "What's next:\u001b[0m\n",
      "    Try Docker Debug for seamless, persistent debugging tools in any container or image → \u001b[36mdocker debug vault-enterprise\u001b[0m\n",
      "    Learn more at https://docs.docker.com/go/debug-cli/\n"
     ]
    }
   ],
   "source": [
    "# As Vault will be connecting to the K8s cluster (minikube), we will need to make sure that it is connecting on a valid URL\n",
    "# as the minikube CA has configured the following DNS entries on the certificate:\n",
    "# control-plane.minikube.internal, kubernetes.default.svc.cluster.local, kubernetes.default.svc, kubernetes.default, kubernetes, localhost\n",
    "# For this demo, we will use \"kubernetes.default\"\n",
    "\n",
    "# First Get the IP address of host.docker.internal using nslookup\n",
    "# last sed command is to strip the extra ^M character at the end of the string.\n",
    "# export DOCKERHOSTIP=$(docker exec -it vault-enterprise nslookup host.docker.internal | tail -n +3 | sed -n 's/Address:\\s*//p' | sed 's/.$//')\n",
    "export DOCKERHOSTIP=$(docker exec -it vault-enterprise nslookup host.docker.internal | tail -n +3 | sed -n 's/Address:\\s*//p' | sed 's/.$//' | grep -Eo '([0-9]{1,3}\\.){3}[0-9]{1,3}')\n",
    "echo \"Docker Host IP address is: $DOCKERHOSTIP\"\n",
    "# Add the IP of host.docker.internal to the /etc/hosts file and map it to kubernetes.default\n",
    "echo \"Updating Vault's /etc/hosts file to add IP of host.docker.internal and mapping it to kubernetes.default\" \n",
    "docker exec -it -e DOCKERHOSTIP=\"$DOCKERHOSTIP\" vault-enterprise /bin/sh -c \"echo $DOCKERHOSTIP kubernetes.default >> /etc/hosts\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:45:58.957486Z",
     "start_time": "2024-10-11T02:45:57.537381Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token for \"vault-svc-account\" service account in \"demo-ns\" namespace is:\n",
      "eyJhbGciOiJSUzI1NiIsImtpZCI6IkxPUTRBdlliNXZRZ3hZeHFva3ZLTnBDeGlPRk9UUXRNYkRvSzFUZEZWczAifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZW1vLW5zIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InZhdWx0LXRva2VuLWRlbW8iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoidmF1bHQtc3ZjLWFjY291bnQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIzYzJkODU1NC0wMzBjLTRiZTktOWY5NS01NDZlMTQ1M2Y4ZjgiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6ZGVtby1uczp2YXVsdC1zdmMtYWNjb3VudCJ9.Nw37f4wch-KW_h5O4I3ZLCApMD2jd65d4lReRrPiSxUmONGMfLSgE-AV4173JV-QvrNLZDiIOs5DHTi798AGQb4Tnrt7HaXpW87Q-65qgEjGkJFgoG5b-eAHIcq5Vxs03l9NYRTnET04nHpS_gwun4QaVmZAc-f6V-EmD0S_qlszqAaoRDiJzRPDWlV4dZeF5Y9JeEaty2XPfHQtqNjyIU6J3vCJKc98ogCAbI0Z-MU1DHMAgVPkU8Jbe8XBnaFGj2oDw1z5Wyekc1_NDFYmnw5yZpJFJgFItgxwSW2jPwRrtel1iWQLSzp7ZVyaTEWPcZbxtZmCtgI7Up_WpG_40A\n",
      "\n",
      "https://127.0.0.1:52324\n",
      "Kubernetes API server - Internal: https://127.0.0.1:52324\n",
      "Kubernetes API server - External (used by Vault): https://kubernetes.default:52324\n",
      "Kubernetes CA Cert: -----BEGIN CERTIFICATE-----\n",
      "MIIDBjCCAe6gAwIBAgIBATANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwptaW5p\n",
      "a3ViZUNBMB4XDTI0MDIxODAzNDIwNFoXDTM0MDIxNjAzNDIwNFowFTETMBEGA1UE\n",
      "AxMKbWluaWt1YmVDQTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALeo\n",
      "g9Pzqjz4VFIskmZc2dR2yCbyXw/2YdY7WDYkLMAHtvJti6pGOk8vCu2vlR9qnS0b\n",
      "9A26d6bqXg9LFZCUXoMWi8FY5rTEhxiwpwbvpwrvO1mnNaj7kUrFVHik/Ieo4u+6\n",
      "yYYeds6BB/PJAJ5b2HOy7YuJU09DzMk8ds73+3nqaOIOz/aw6mf0CxxeoUsBeFIp\n",
      "bgrCibUk1PxguwUR2qQoW2zFuwLd8bgZewKSmJSxf4DBpxDlPuEwRDfI5O4yeqaP\n",
      "Q/huq0nyQ+FHAv5kOn8S8/ShWQjOnR6M1Z075Ux+9mVtzn2M1sX+2FJFs0X1ZVCd\n",
      "OLf+66X53WH5GqjZpy8CAwEAAaNhMF8wDgYDVR0PAQH/BAQDAgKkMB0GA1UdJQQW\n",
      "MBQGCCsGAQUFBwMCBggrBgEFBQcDATAPBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQW\n",
      "BBTjOba6uK36Fp2EqRX9lUhWlF3IVjANBgkqhkiG9w0BAQsFAAOCAQEAm87v2gdw\n",
      "xeHcdIYUWX/LR2GdnXGPBV1Se+kzF6blLAuqiB4F+ckmspyEjOQbbuJrCH/aUz7+\n",
      "s2M8Rn4Liyom1QEx4auYpge0wkYY0fOHfALVdVi7b2Is0C1C6HCFH0i5EAkr4tQf\n",
      "5M+JLgCdfLMcnJDUZRa3MD4rO5hClqDkEmAvV3E3vqWq4X4/K3GcuUwBPLuU3SD5\n",
      "v6/Wzrp0l4AQpHFbiXC/QF3YWquBrDS3opfsbivMH8xvKZkk+EG2QqNLOE29WqoD\n",
      "wx9i9Tog5q4B2Gks8gXvcbRaSHnXj2REllINRIvkCQ7J8q9ke6kA1wMwV1DqLCPl\n",
      "HGU2er7c/AnVWQ==\n",
      "-----END CERTIFICATE-----\n"
     ]
    }
   ],
   "source": [
    "# Prepare information needed for Kubernetes Auth Configuration\n",
    "\n",
    "# 1. Get JWT token for Vault K8s Auth configuration\n",
    "export JWT_TOKEN_DEFAULT_DEMONS=$(kubectl get secret -n $KUBENAMESPACE $KUBESVCACCOUNTTOKEN --output='go-template={{ .data.token }}' | base64 --decode)\n",
    "echo \"Token for \\\"$KUBESVCACCOUNT\\\" service account in \\\"$KUBENAMESPACE\\\" namespace is:\"\n",
    "echo $JWT_TOKEN_DEFAULT_DEMONS\n",
    "echo\n",
    "# You can view the JWT token information using http://calebb.net/\n",
    "\n",
    "# 2. Obtain kubernetes API URL\n",
    "# First get the internal API URL\n",
    "# NOTE: This is currently only taking the first record, which may fail\n",
    "export KUBE_INT_API=$(kubectl config view -o jsonpath='{.clusters[3].cluster.server}')\n",
    "echo $KUBE_INT_API\n",
    "# replace KUBE API external address 127.0.0.1 with configured \"kubernetes.default\" DNS entry.\n",
    "# This will allow docker pods to connect back to the K8s\n",
    "export KUBE_EXT_API=$(echo $KUBE_INT_API | sed \"s/127.0.0.1/kubernetes.default/g\")\n",
    "# Get the K8s CA Cert\n",
    "export KUBE_CA_CERT=$(kubectl config view --raw --minify --flatten --output='jsonpath={.clusters[].cluster.certificate-authority-data}' | base64 --decode)\n",
    "echo \"Kubernetes API server - Internal: $KUBE_INT_API\"\n",
    "echo \"Kubernetes API server - External (used by Vault): $KUBE_EXT_API\"\n",
    "echo \"Kubernetes CA Cert: $KUBE_CA_CERT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:46:07.548124Z",
     "start_time": "2024-10-11T02:46:06.637760Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kubernetes Auth Path: kubernetes-minikube\n",
      "https://kubernetes.default:52324\n",
      "kubernetes-minikube\n",
      "-----BEGIN CERTIFICATE----- MIIDBjCCAe6gAwIBAgIBATANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwptaW5p a3ViZUNBMB4XDTI0MDIxODAzNDIwNFoXDTM0MDIxNjAzNDIwNFowFTETMBEGA1UE AxMKbWluaWt1YmVDQTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALeo g9Pzqjz4VFIskmZc2dR2yCbyXw/2YdY7WDYkLMAHtvJti6pGOk8vCu2vlR9qnS0b 9A26d6bqXg9LFZCUXoMWi8FY5rTEhxiwpwbvpwrvO1mnNaj7kUrFVHik/Ieo4u+6 yYYeds6BB/PJAJ5b2HOy7YuJU09DzMk8ds73+3nqaOIOz/aw6mf0CxxeoUsBeFIp bgrCibUk1PxguwUR2qQoW2zFuwLd8bgZewKSmJSxf4DBpxDlPuEwRDfI5O4yeqaP Q/huq0nyQ+FHAv5kOn8S8/ShWQjOnR6M1Z075Ux+9mVtzn2M1sX+2FJFs0X1ZVCd OLf+66X53WH5GqjZpy8CAwEAAaNhMF8wDgYDVR0PAQH/BAQDAgKkMB0GA1UdJQQW MBQGCCsGAQUFBwMCBggrBgEFBQcDATAPBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQW BBTjOba6uK36Fp2EqRX9lUhWlF3IVjANBgkqhkiG9w0BAQsFAAOCAQEAm87v2gdw xeHcdIYUWX/LR2GdnXGPBV1Se+kzF6blLAuqiB4F+ckmspyEjOQbbuJrCH/aUz7+ s2M8Rn4Liyom1QEx4auYpge0wkYY0fOHfALVdVi7b2Is0C1C6HCFH0i5EAkr4tQf 5M+JLgCdfLMcnJDUZRa3MD4rO5hClqDkEmAvV3E3vqWq4X4/K3GcuUwBPLuU3SD5 v6/Wzrp0l4AQpHFbiXC/QF3YWquBrDS3opfsbivMH8xvKZkk+EG2QqNLOE29WqoD wx9i9Tog5q4B2Gks8gXvcbRaSHnXj2REllINRIvkCQ7J8q9ke6kA1wMwV1DqLCPl HGU2er7c/AnVWQ== -----END CERTIFICATE-----\n",
      "\u001b[0mSuccess! Data written to: auth/kubernetes-minikube/config\u001b[0m\n",
      "\u001b[0mKey                                  Value\n",
      "---                                  -----\n",
      "disable_iss_validation               true\n",
      "disable_local_ca_jwt                 false\n",
      "issuer                               n/a\n",
      "kubernetes_ca_cert                   -----BEGIN CERTIFICATE-----\n",
      "MIIDBjCCAe6gAwIBAgIBATANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwptaW5p\n",
      "a3ViZUNBMB4XDTI0MDIxODAzNDIwNFoXDTM0MDIxNjAzNDIwNFowFTETMBEGA1UE\n",
      "AxMKbWluaWt1YmVDQTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALeo\n",
      "g9Pzqjz4VFIskmZc2dR2yCbyXw/2YdY7WDYkLMAHtvJti6pGOk8vCu2vlR9qnS0b\n",
      "9A26d6bqXg9LFZCUXoMWi8FY5rTEhxiwpwbvpwrvO1mnNaj7kUrFVHik/Ieo4u+6\n",
      "yYYeds6BB/PJAJ5b2HOy7YuJU09DzMk8ds73+3nqaOIOz/aw6mf0CxxeoUsBeFIp\n",
      "bgrCibUk1PxguwUR2qQoW2zFuwLd8bgZewKSmJSxf4DBpxDlPuEwRDfI5O4yeqaP\n",
      "Q/huq0nyQ+FHAv5kOn8S8/ShWQjOnR6M1Z075Ux+9mVtzn2M1sX+2FJFs0X1ZVCd\n",
      "OLf+66X53WH5GqjZpy8CAwEAAaNhMF8wDgYDVR0PAQH/BAQDAgKkMB0GA1UdJQQW\n",
      "MBQGCCsGAQUFBwMCBggrBgEFBQcDATAPBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQW\n",
      "BBTjOba6uK36Fp2EqRX9lUhWlF3IVjANBgkqhkiG9w0BAQsFAAOCAQEAm87v2gdw\n",
      "xeHcdIYUWX/LR2GdnXGPBV1Se+kzF6blLAuqiB4F+ckmspyEjOQbbuJrCH/aUz7+\n",
      "s2M8Rn4Liyom1QEx4auYpge0wkYY0fOHfALVdVi7b2Is0C1C6HCFH0i5EAkr4tQf\n",
      "5M+JLgCdfLMcnJDUZRa3MD4rO5hClqDkEmAvV3E3vqWq4X4/K3GcuUwBPLuU3SD5\n",
      "v6/Wzrp0l4AQpHFbiXC/QF3YWquBrDS3opfsbivMH8xvKZkk+EG2QqNLOE29WqoD\n",
      "wx9i9Tog5q4B2Gks8gXvcbRaSHnXj2REllINRIvkCQ7J8q9ke6kA1wMwV1DqLCPl\n",
      "HGU2er7c/AnVWQ==\n",
      "-----END CERTIFICATE-----\n",
      "kubernetes_host                      https://kubernetes.default:52324\n",
      "pem_keys                             []\n",
      "token_reviewer_jwt_set               true\n",
      "use_annotations_as_alias_metadata    false\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Configure Kubernetes Auth for the minikube cluster\n",
    "echo \"Kubernetes Auth Path: $K8SAUTHPATH\"\n",
    "echo $KUBE_EXT_API\n",
    "echo $K8SAUTHPATH\n",
    "echo $KUBE_CA_CERT\n",
    "\n",
    "vault write auth/$K8SAUTHPATH/config \\\n",
    "    token_reviewer_jwt=\"$JWT_TOKEN_DEFAULT_DEMONS\" \\\n",
    "    kubernetes_host=\"$KUBE_EXT_API\" \\\n",
    "    kubernetes_ca_cert=\"$KUBE_CA_CERT\"\n",
    "#     kubernetes_ca_cert=@~/.minikube/ca.crt  # Another option of specifying the CA cert if using minikube\n",
    "\n",
    "vault read auth/$K8SAUTHPATH/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:46:17.286165Z",
     "start_time": "2024-10-11T02:46:15.751995Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mSuccess! Uploaded policy: policy-readonly-db\u001b[0m\n",
      "K8s Service Account: vault-svc-account\n",
      "K8s namespace: demo-ns\n",
      "K8s role: auth-role-minikube\n",
      "\u001b[0mSuccess! Data written to: auth/kubernetes-minikube/role/auth-role-minikube\u001b[0m\n",
      "\u001b[0mKey                                         Value\n",
      "---                                         -----\n",
      "alias_name_source                           serviceaccount_uid\n",
      "audience                                    vault\n",
      "bound_service_account_names                 [vault-svc-account]\n",
      "bound_service_account_namespace_selector    n/a\n",
      "bound_service_account_namespaces            [demo-ns]\n",
      "period                                      1m\n",
      "token_bound_cidrs                           []\n",
      "token_explicit_max_ttl                      0s\n",
      "token_max_ttl                               0s\n",
      "token_no_default_policy                     false\n",
      "token_num_uses                              0\n",
      "token_period                                1m\n",
      "token_policies                              [policy-readonly-db]\n",
      "token_ttl                                   0s\n",
      "token_type                                  default\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Create vault policy to be used by the K8s role to read database dynamic secret\n",
    "vault policy write policy-readonly-db - <<EOF\n",
    "path \"$DBPATH/creds/$PGROLE\" {\n",
    "   capabilities = [\"read\"]\n",
    "}\n",
    "\n",
    "path \"auth/token/*\" {\n",
    "  capabilities = [\"create\", \"update\"]\n",
    "}\n",
    "\n",
    "path \"auth/*\" { \n",
    "  capabilities = [\"read\", \"create\", \"update\"]\n",
    "}\n",
    "\n",
    "path \"auth/approle/role/*\" { \n",
    "  capabilities = [\"read\", \"create\", \"update\"]\n",
    "}\n",
    "\n",
    "path \"auth/approle/role/agent-app-role/*\" { \n",
    "  capabilities = [\"read\", \"create\", \"update\"]\n",
    "}\n",
    "\n",
    "path \"auth/approle/role/agent-app-role/secret-id\" {\n",
    "  capabilities = [\"read\", \"create\", \"update\"]\n",
    "}\n",
    "EOF\n",
    "\n",
    "# Create a new role for the dynamic secret with the required policy\n",
    "# You can configure the TTL and max TTL at the role level.  \n",
    "# For this demo, we will be setting the K8s auth role's token period to 60s\n",
    "# This ensures the token has no max TTL but renewals is based on the token period\n",
    "# https://developer.hashicorp.com/vault/api-docs/auth/token#period\n",
    "echo \"K8s Service Account: $KUBESVCACCOUNT\"\n",
    "echo \"K8s namespace: $KUBENAMESPACE\"\n",
    "echo \"K8s role: $K8SROLE\"\n",
    "vault write auth/$K8SAUTHPATH/role/$K8SROLE \\\n",
    "   bound_service_account_names=$KUBESVCACCOUNT \\\n",
    "   bound_service_account_namespaces=$KUBENAMESPACE \\\n",
    "   period=60 \\\n",
    "   token_policies=policy-readonly-db\n",
    "\n",
    "# show values\n",
    "vault read auth/$K8SAUTHPATH/role/$K8SROLE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:46:44.304899Z",
     "start_time": "2024-10-11T02:46:43.493440Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JWT Token used for login:\n",
      "eyJhbGciOiJSUzI1NiIsImtpZCI6IkxPUTRBdlliNXZRZ3hZeHFva3ZLTnBDeGlPRk9UUXRNYkRvSzFUZEZWczAifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZW1vLW5zIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InZhdWx0LXRva2VuLWRlbW8iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoidmF1bHQtc3ZjLWFjY291bnQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIzYzJkODU1NC0wMzBjLTRiZTktOWY5NS01NDZlMTQ1M2Y4ZjgiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6ZGVtby1uczp2YXVsdC1zdmMtYWNjb3VudCJ9.Nw37f4wch-KW_h5O4I3ZLCApMD2jd65d4lReRrPiSxUmONGMfLSgE-AV4173JV-QvrNLZDiIOs5DHTi798AGQb4Tnrt7HaXpW87Q-65qgEjGkJFgoG5b-eAHIcq5Vxs03l9NYRTnET04nHpS_gwun4QaVmZAc-f6V-EmD0S_qlszqAaoRDiJzRPDWlV4dZeF5Y9JeEaty2XPfHQtqNjyIU6J3vCJKc98ogCAbI0Z-MU1DHMAgVPkU8Jbe8XBnaFGj2oDw1z5Wyekc1_NDFYmnw5yZpJFJgFItgxwSW2jPwRrtel1iWQLSzp7ZVyaTEWPcZbxtZmCtgI7Up_WpG_40A\n",
      "auth-role-minikube\n",
      "kubernetes-minikube\n",
      "K8s Service Account: vault-svc-account\n",
      "K8s namespace: demo-ns\n",
      "K8s role: auth-role-minikube\n",
      "\u001b[0mKey                                       Value\n",
      "---                                       -----\n",
      "token                                     hvs.CAESILU-NqgKdDelkcU8CYQN3epSa85kDawlFcWKB86yXufGGh4KHGh2cy5oTExlUFA1VVN0ZjlFU1czanllTHV2OGM\n",
      "token_accessor                            Q7dFgob9Llr48BWyvrWUVZI8\n",
      "token_duration                            1m\n",
      "token_renewable                           true\n",
      "token_policies                            [\"default\" \"policy-readonly-db\"]\n",
      "identity_policies                         []\n",
      "policies                                  [\"default\" \"policy-readonly-db\"]\n",
      "token_meta_role                           auth-role-minikube\n",
      "token_meta_service_account_name           vault-svc-account\n",
      "token_meta_service_account_namespace      demo-ns\n",
      "token_meta_service_account_secret_name    vault-token-demo\n",
      "token_meta_service_account_uid            3c2d8554-030c-4be9-9f95-546e1453f8f8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Test that we can login\n",
    "echo \"JWT Token used for login:\"\n",
    "echo $JWT_TOKEN_DEFAULT_DEMONS\n",
    "echo $K8SROLE\n",
    "echo $K8SAUTHPATH\n",
    "echo \"K8s Service Account: $KUBESVCACCOUNT\"\n",
    "echo \"K8s namespace: $KUBENAMESPACE\"\n",
    "echo \"K8s role: $K8SROLE\"\n",
    "\n",
    "vault write auth/$K8SAUTHPATH/login role=\"$K8SROLE\" jwt=$JWT_TOKEN_DEFAULT_DEMONS\n",
    "\n",
    "# If this fails, please review the docker logs for the vault-enterprise container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT   NAME       CLUSTER    AUTHINFO   NAMESPACE\n",
      "*         minikube   minikube   minikube   default\n"
     ]
    }
   ],
   "source": [
    "# Debugging \n",
    "kubectl config get-contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    " # Debugging\n",
    " kubectl config delete-context arn:aws:eks:ap-southeast-1:058264549112:cluster/boundary-ent-3kyocw5i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:47:03.201166Z",
     "start_time": "2024-10-11T02:47:02.752401Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K8s Service Account: vault-svc-account\n",
      "K8s namespace: demo-ns\n",
      "K8s role: auth-role-minikube\n",
      "\u001b[0mSuccess! Data written to: auth/kubernetes-minikube/role/auth-role-minikube\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Update the role to include the audience claim value to \"vault\".  This will be used as part of the VSO setup\n",
    "echo \"K8s Service Account: $KUBESVCACCOUNT\"\n",
    "echo \"K8s namespace: $KUBENAMESPACE\"\n",
    "echo \"K8s role: $K8SROLE\"\n",
    "vault write auth/$K8SAUTHPATH/role/$K8SROLE \\\n",
    "   audience=vault"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the Vault Secrets Operator helm chart to your K8s cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:47:14.013595Z",
     "start_time": "2024-10-11T02:47:13.054550Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Run K9s to monitor K8s cluster. https://k9scli.io/\n",
    "# Some quick commands\n",
    "# :po - View pods\n",
    "# :sa - View service accounts\n",
    "# :ns - View namespace\n",
    "# :sec - View secrets\n",
    "# :crd - View CRDs\n",
    "# :deploy - View deployments\n",
    "# :q - quit\n",
    "# 0 - View all namespaces.  Type other numbers for specific namespaces.\n",
    "open /opt/homebrew/bin/k9s\n",
    "\n",
    "# Start by viewing secrets and you can see the service-account-token that was created earlier. \n",
    "# Type :sec and type '0' to view all secrets in all namespaces.\n",
    "# Press Enter to view the details and escape to return back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"hashicorp\" already exists with the same configuration, skipping\n"
     ]
    }
   ],
   "source": [
    "# Add the HashiCorp repo (Only required for the first time)\n",
    "helm repo add hashicorp https://helm.releases.hashicorp.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hang tight while we grab the latest from your chart repositories...\n",
      "...Successfully got an update from the \"hashicorp\" chart repository\n",
      "...Successfully got an update from the \"bitnami\" chart repository\n",
      "Update Complete. ⎈Happy Helming!⎈\n",
      "NAME                            \tCHART VERSION\tAPP VERSION\tDESCRIPTION                          \n",
      "hashicorp/vault-secrets-operator\t0.10.0       \t0.10.0     \tOfficial Vault Secrets Operator Chart\n",
      "hashicorp/vault-secrets-operator\t0.9.1        \t0.9.1      \tOfficial Vault Secrets Operator Chart\n",
      "hashicorp/vault-secrets-operator\t0.9.0        \t0.9.0      \tOfficial Vault Secrets Operator Chart\n",
      "hashicorp/vault-secrets-operator\t0.8.1        \t0.8.1      \tOfficial Vault Secrets Operator Chart\n",
      "hashicorp/vault-secrets-operator\t0.8.0        \t0.8.0      \tOfficial Vault Secrets Operator Chart\n",
      "hashicorp/vault-secrets-operator\t0.7.1        \t0.7.1      \tOfficial Vault Secrets Operator Chart\n",
      "hashicorp/vault-secrets-operator\t0.7.0        \t0.7.0      \tOfficial Vault Secrets Operator Chart\n",
      "hashicorp/vault-secrets-operator\t0.6.0        \t0.6.0      \tOfficial Vault Secrets Operator Chart\n",
      "hashicorp/vault-secrets-operator\t0.5.2        \t0.5.2      \tOfficial Vault Secrets Operator Chart\n",
      "hashicorp/vault-secrets-operator\t0.5.1        \t0.5.1      \tOfficial Vault Secrets Operator Chart\n",
      "hashicorp/vault-secrets-operator\t0.5.0        \t0.5.0      \tOfficial Vault Secrets Operator Chart\n",
      "hashicorp/vault-secrets-operator\t0.4.3        \t0.4.3      \tOfficial Vault Secrets Operator Chart\n",
      "hashicorp/vault-secrets-operator\t0.4.2        \t0.4.2      \tOfficial Vault Secrets Operator Chart\n",
      "hashicorp/vault-secrets-operator\t0.4.1        \t0.4.1      \tOfficial Vault Secrets Operator Chart\n",
      "hashicorp/vault-secrets-operator\t0.4.0        \t0.4.0      \tOfficial Vault Secrets Operator Chart\n",
      "hashicorp/vault-secrets-operator\t0.3.4        \t0.3.4      \tOfficial Vault Secrets Operator Chart\n",
      "hashicorp/vault-secrets-operator\t0.3.3        \t0.3.3      \tOfficial Vault Secrets Operator Chart\n",
      "hashicorp/vault-secrets-operator\t0.3.2        \t0.3.2      \tOfficial Vault Secrets Operator Chart\n",
      "hashicorp/vault-secrets-operator\t0.3.1        \t0.3.1      \tOfficial Vault Secrets Operator Chart\n",
      "hashicorp/vault-secrets-operator\t0.2.0        \t0.2.0      \tOfficial Vault Secrets Operator Chart\n",
      "hashicorp/vault-secrets-operator\t0.1.0        \t0.1.0      \tOfficial Vault Secrets Operator Chart\n"
     ]
    }
   ],
   "source": [
    "# Optional.  Update the repo (Only required when new versions are released)\n",
    "# Vault Secrets Operator supports for the latest three versions of Vault.\n",
    "helm repo update\n",
    "# Check latest versions\n",
    "helm search repo hashicorp/vault-secrets-operator -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:47:30.563153Z",
     "start_time": "2024-10-11T02:47:24.337334Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vault Server Address: http://host.minikube.internal:8200\n",
      "NAME: vault-secrets-operator\n",
      "LAST DEPLOYED: Tue Mar 18 14:55:20 2025\n",
      "NAMESPACE: vault-secrets-operator-system\n",
      "STATUS: deployed\n",
      "REVISION: 1\n"
     ]
    }
   ],
   "source": [
    "# Install the VSO helm chart and specify the Vault server address\n",
    "echo \"Vault Server Address: $MINIKUBEVAULTADDRESS\"\n",
    "\n",
    "helm install vault-secrets-operator hashicorp/vault-secrets-operator \\\n",
    "--version 0.6.0 -n vault-secrets-operator-system --create-namespace --values - <<EOF\n",
    "defaultVaultConnection:\n",
    "  # toggles the deployment of the VaultAuthMethod CR\n",
    "  enabled: true\n",
    "  # Address of the Vault Server.\n",
    "  address: $MINIKUBEVAULTADDRESS\n",
    "  skipTLSVerify: true\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:47:33.278656Z",
     "start_time": "2024-10-11T02:47:32.813206Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                  \tNAMESPACE                    \tREVISION\tUPDATED                             \tSTATUS  \tCHART                       \tAPP VERSION\n",
      "vault-secrets-operator\tvault-secrets-operator-system\t1       \t2025-03-18 14:55:20.874479 +0800 +08\tdeployed\tvault-secrets-operator-0.6.0\t0.6.0      \n",
      "NAME                                                         READY   STATUS              RESTARTS   AGE\n",
      "vault-secrets-operator-controller-manager-79b6cff8b4-nsmms   0/2     ContainerCreating   0          3s\n"
     ]
    }
   ],
   "source": [
    "# View installed charts\n",
    "helm list -A\n",
    "# Check that the two VSO pods are ready before proceeding\n",
    "kubectl get pods -n vault-secrets-operator-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:47:39.544645Z",
     "start_time": "2024-10-11T02:47:38.653363Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K8s namespace: demo-ns\n",
      "Vault Server Address: http://host.minikube.internal:8200\n",
      "vaultconnection.secrets.hashicorp.com/vault-connection-demo created\n"
     ]
    }
   ],
   "source": [
    "# Setting up for VaultConnection CRD.  This will be used by the VaultAuth CRD below\n",
    "echo \"K8s namespace: $KUBENAMESPACE\"\n",
    "echo \"Vault Server Address: $MINIKUBEVAULTADDRESS\"\n",
    "\n",
    "kubectl create -f - <<EOF\n",
    "apiVersion: secrets.hashicorp.com/v1beta1\n",
    "kind: VaultConnection\n",
    "metadata:\n",
    "  name: vault-connection-demo\n",
    "  namespace: $KUBENAMESPACE\n",
    "spec:\n",
    "  address: $MINIKUBEVAULTADDRESS\n",
    "  skipTLSVerify: true\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:47:52.112743Z",
     "start_time": "2024-10-11T02:47:50.690166Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K8s namespace: demo-ns\n",
      "K8s Auth Path: kubernetes-minikube\n",
      "K8s role: auth-role-minikube\n",
      "K8s Service Account: vault-svc-account\n",
      "Error from server (AlreadyExists): error when creating \"STDIN\": vaultauths.secrets.hashicorp.com \"vault-auth-demo\" already exists\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "# Setting up VaultAuth.  Make sure the \"mount\" and \"role name\" matches the mount/role created in Vault.\n",
    "# vaultConnectionRef references the VaultConnection above\n",
    "# Also note the audiences specification here is set to \"vault\" and matches with the audiences setting in the configured role for Vault K8s Auth\n",
    "echo \"K8s namespace: $KUBENAMESPACE\"\n",
    "echo \"K8s Auth Path: $K8SAUTHPATH\"\n",
    "echo \"K8s role: $K8SROLE\"\n",
    "echo \"K8s Service Account: $KUBESVCACCOUNT\"\n",
    "\n",
    "kubectl create -f - <<EOF\n",
    "apiVersion: secrets.hashicorp.com/v1beta1\n",
    "kind: VaultAuth\n",
    "metadata:\n",
    "  name: vault-auth-demo\n",
    "  namespace: $KUBENAMESPACE\n",
    "spec:\n",
    "  vaultConnectionRef: vault-connection-demo\n",
    "  method: kubernetes\n",
    "  mount: $K8SAUTHPATH\n",
    "  kubernetes:\n",
    "    role: $K8SROLE\n",
    "    serviceAccount: $KUBESVCACCOUNT\n",
    "    audiences:\n",
    "      - vault\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d9d6b340-1aca-ff73-911a-c54ec42d9d42\n",
      "Error from server (AlreadyExists): error when creating \"STDIN\": vaultauths.secrets.hashicorp.com \"vault-auth-approle\" already exists\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "export ROLEID=$(cat roleid)\n",
    "echo $ROLEID\n",
    "\n",
    "kubectl create -f - <<EOF\n",
    "apiVersion: secrets.hashicorp.com/v1beta1\n",
    "kind: VaultAuth\n",
    "metadata:\n",
    "  name: vault-auth-approle\n",
    "  namespace: $KUBENAMESPACE\n",
    "spec:\n",
    "  vaultConnectionRef: vault-connection-demo\n",
    "  method: appRole\n",
    "  mount: approle\n",
    "  appRole: \n",
    "    roleId: $ROLEID\n",
    "    secretRef: secretkv-approle\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:48:11.416321Z",
     "start_time": "2024-10-11T02:48:08.776257Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K8s namespace: demo-ns\n",
      "Vault K8s auth mount path: database\n",
      "Vault role: readonly\n",
      "vaultdynamicsecret.secrets.hashicorp.com/vso-db-demo created\n"
     ]
    }
   ],
   "source": [
    "# Create the VaultDynamicSecret CRD.  \n",
    "# This will create the K8s secret and bind it to sync with the dynamic database secret.\n",
    "# Other supported options are VaultStaticSecret and VaultPKISecret\n",
    "# Ref: https://developer.hashicorp.com/vault/docs/platform/k8s/vso/sources/vault#vault-secret-custom-resource-definitions\n",
    "echo \"K8s namespace: $KUBENAMESPACE\"\n",
    "echo \"Vault K8s auth mount path: $DBPATH\"\n",
    "echo \"Vault role: $PGROLE\"\n",
    "\n",
    "kubectl create -f - <<EOF\n",
    "apiVersion: secrets.hashicorp.com/v1beta1\n",
    "kind: VaultDynamicSecret\n",
    "metadata:\n",
    "  name: vso-db-demo\n",
    "  namespace: $KUBENAMESPACE\n",
    "spec:\n",
    "\n",
    "  # Mount path of the secrets backend\n",
    "  mount: $DBPATH\n",
    "\n",
    "  # Path to the secret\n",
    "  path: creds/$PGROLE\n",
    "\n",
    "  # Where to store the secrets, \n",
    "  # If create is false, end user will create the secret.  \n",
    "  # If create is true, VSO will create the secret\n",
    "  destination:\n",
    "    create: true\n",
    "    name: vso-db-demo\n",
    "\n",
    "  # Restart these pods when secrets rotated\n",
    "  rolloutRestartTargets:\n",
    "  - kind: Deployment\n",
    "    name: vso-db-demo\n",
    "\n",
    "  # Name of the CRD to authenticate to Vault\n",
    "  vaultAuthRef: vault-auth-demo\n",
    "EOF\n",
    "\n",
    "# If create: false, you will need to create the secret manually.  Example below.\n",
    "# kubectl create -f - <<EOF\n",
    "# apiVersion: v1\n",
    "# kind: Secret\n",
    "# metadata:\n",
    "#   name: vso-db-demo\n",
    "#   namespace: $KUBENAMESPACE\n",
    "# EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K8s namespace: demo-ns\n",
      "vaultdynamicsecret.secrets.hashicorp.com/secretkv-approle created\n"
     ]
    }
   ],
   "source": [
    "echo \"K8s namespace: $KUBENAMESPACE\"\n",
    "\n",
    "kubectl create -f - <<EOF\n",
    "apiVersion: secrets.hashicorp.com/v1beta1\n",
    "kind: VaultDynamicSecret\n",
    "metadata:\n",
    "  name: secretkv-approle\n",
    "  namespace: $KUBENAMESPACE\n",
    "spec:\n",
    "  # Mount path of the secrets backend\n",
    "  mount: auth\n",
    "\n",
    "  # Path to the secret\n",
    "  path: approle/role/agent-app-role/secret-id\n",
    "  requestHTTPMethod: POST\n",
    "\n",
    "  # Where to store the secrets, \n",
    "  # If create is false, end user will create the secret.  \n",
    "  # If create is true, VSO will create the secret\n",
    "  destination:\n",
    "    name: secretkv-approle\n",
    "    create: false\n",
    "    transformation:\n",
    "      excludes:\n",
    "        - .*\n",
    "      templates:\n",
    "        id:\n",
    "          text: \"{{- .Secrets.secret_id -}}\"\n",
    "          \n",
    "  refreshAfter: 1m\n",
    "\n",
    "  # Name of the CRD to authenticate to Vault\n",
    "  vaultAuthRef: vault-auth-approle\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:48:28.022855Z",
     "start_time": "2024-10-11T02:48:26.904360Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Let's have a look at the dynamic credentials in K8s secrets\n",
    "# You can also use k9s to view the secret.\n",
    "# - Use ':sec' to view secrets.  Type '0' to view all namespaces.\n",
    "# - Use the up and down arrows to select the vso demo secret name and then press 'x' to view the decoded secret.\n",
    "echo $KUBENAMESPACE\n",
    "echo \"K8s dynamic secret name: $(kubectl get secrets -n $KUBENAMESPACE --output=json | jq -r '.items[] | select(.data.username).metadata.name')\"\n",
    "export DYNAMICUSERNAME=$(kubectl get secrets -n $KUBENAMESPACE --output=json | jq -r '.items[] | .data | select(.username) | .username')\n",
    "export DYNAMICPASSWORD=$(kubectl get secrets -n $KUBENAMESPACE --output=json | jq -r '.items[] | .data | select(.username) | .password')\n",
    "echo\n",
    "echo \"Dynamic credentials (base64 encoded):\"\n",
    "echo \"username: $DYNAMICUSERNAME\"\n",
    "echo \"password: $DYNAMICPASSWORD\"\n",
    "echo\n",
    "echo \"Dynamic credentials (decoded):\"\n",
    "echo \"username: $(echo $DYNAMICUSERNAME | base64 --decode)\"\n",
    "echo \"password: $(echo $DYNAMICPASSWORD | base64 --decode)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:49:17.280663Z",
     "start_time": "2024-10-11T02:49:13.472153Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Create new application nginx deployment that references the K8s dynamic secret\n",
    "# Also note that this shows two ways of binding the secrets\n",
    "# - via env: (Environment Variables)\n",
    "# - via volumeMounts: (Volume Mount)\n",
    "# View the vault-enterprise pod logs and also the k9s pods dashboard - type :po\n",
    "\n",
    "kubectl create -f - <<EOF\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: vso-db-demo\n",
    "  namespace: $KUBENAMESPACE\n",
    "  labels:\n",
    "    test: vso-db-demo\n",
    "spec:\n",
    "  # Change this to indicate how many pods to provision.  We will be testing with 3 pods.\n",
    "  replicas: 3\n",
    "  strategy:\n",
    "    rollingUpdate:\n",
    "      maxUnavailable: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      test: vso-db-demo\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        test: vso-db-demo\n",
    "    spec:\n",
    "      volumes:\n",
    "        - name: secrets\n",
    "          secret:\n",
    "            secretName: \"vso-db-demo\"\n",
    "      containers:\n",
    "        - name: example\n",
    "          image: nginx:latest\n",
    "          env:\n",
    "            - name: DB_PASSWORD\n",
    "              valueFrom:\n",
    "                secretKeyRef:\n",
    "                  name: \"vso-db-demo\"\n",
    "                  key: password\n",
    "            - name: DB_USERNAME\n",
    "              valueFrom:\n",
    "                secretKeyRef:\n",
    "                  name: \"vso-db-demo\"\n",
    "                  key: username\n",
    "          volumeMounts:\n",
    "            - name: secrets\n",
    "              mountPath: /etc/secrets\n",
    "              readOnly: true\n",
    "          resources:\n",
    "            limits:\n",
    "              cpu: \"0.5\"\n",
    "              memory: \"512Mi\"\n",
    "            requests:\n",
    "              cpu: \"250m\"\n",
    "              memory: \"50Mi\"\n",
    "          livenessProbe:\n",
    "            httpGet:\n",
    "              path: /\n",
    "              port: 80\n",
    "              httpHeaders:\n",
    "                - name: X-Custom-Header\n",
    "                  value: Awesome\n",
    "            initialDelaySeconds: 3\n",
    "            periodSeconds: 3\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:49:57.645233Z",
     "start_time": "2024-10-11T02:49:56.184494Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Access one of the application pods and access the secret\n",
    "# Show that the secret is mounted by volume in /etc/secrets\n",
    "# and also as environment variables\n",
    "export APPPODNAME=$(kubectl get pods -n demo-ns | grep -m 1 demo | cut -d \" \" -f1)\n",
    "echo \"Pod: $APPPODNAME\"\n",
    "echo -e \"\\n/etc/secrets/username:\"\n",
    "kubectl exec -it -n $KUBENAMESPACE $APPPODNAME -- /bin/sh -c 'cat /etc/secrets/username'\n",
    "echo -e \"\\n/etc/secrets/password:\"\n",
    "kubectl exec -it -n $KUBENAMESPACE $APPPODNAME -- /bin/sh -c 'cat /etc/secrets/password'\n",
    "echo -e \"\\n\\nEnvironment Variables:\"\n",
    "kubectl exec -it -n $KUBENAMESPACE $APPPODNAME -- /bin/sh -c 'env | grep DB_USERNAME'\n",
    "kubectl exec -it -n $KUBENAMESPACE $APPPODNAME -- /bin/sh -c 'env | grep DB_PASSWORD'\n",
    "\n",
    "# You should also notice in K9s that the application pods gets recreated when the secret lease max TTL is reached.\n",
    "# The secret is revoked and a new secret is generated. \n",
    "\n",
    "# You can also use k9s to view the VSO controller pod logs to see the activity of the renewal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup VSO Persistent Caching with Vault Transit Encryption (Optional but Highly Recommended)\n",
    "\n",
    "In the case of dynamic secrets, we highly recommend that VSO run with persistence caching enabled for performance reasons. This allow for a new VSO leader to pick up where the old VSO leader left off.  This ensures that any Vault tokens used to sync dynamic secrets are renewed.\n",
    "\n",
    "If there is no persistence caching all dynamic secret leases would be revoked when the Vault token TTL expires.\n",
    "See https://developer.hashicorp.com/vault/docs/concepts/lease\n",
    "\n",
    "The transit encryption engine is used to encrypt the persistent Vault client cache. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# To test the VSO persistent caching, we will be tweaking the database role\n",
    "# TTL 90 secs and the max TTL to 180 secs for testing\n",
    "echo \"Database Engine Path is: $DBPATH\"\n",
    "echo \"Postgres Role is: $PGROLE\"\n",
    "vault write $DBPATH/roles/$PGROLE \\\n",
    "      default_ttl=90 \\\n",
    "      max_ttl=180\n",
    "\n",
    "# Show role settings\n",
    "vault read $DBPATH/roles/$PGROLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Now we will see the behavior when the VSO controller is restarted. \n",
    "# As the token is not cached, when the current token expires, the leases to the secrets expire as well.\n",
    "\n",
    "# Wait until the app pods are recreated before restarting the VSO controller.\n",
    "\n",
    "# Stop VSO controller\n",
    "kubectl scale deployment -n vault-secrets-operator-system vault-secrets-operator-controller-manager --replicas=0\n",
    "# Start VSO controller\n",
    "kubectl scale deployment -n vault-secrets-operator-system vault-secrets-operator-controller-manager --replicas=1\n",
    "\n",
    "# You will notice the the app pods will restart before the max TTL is reached as the VSO controller needs to retrieve a new vault token.\n",
    "# The corresponding database secret leases will also expire and needs to be refreshed.\n",
    "\n",
    "# Use k9s to view the vault-secret-operator-controller-manager pod logs in the vault-secrets-operator-system namespace\n",
    "# You should see the clientCachePersistenceModel is set to \"none\"\n",
    "# and you should see the secret being renewed and a new secret being synced to K8s.\n",
    "# This will trigger a restart on the application pods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Configure the path of the transit engine\n",
    "export TRANSITPATH=transit\n",
    "export ENCKEYNAME=vso-client-cache\n",
    "# Role name to be used by VSO to encrypt client cache storage\n",
    "export K8SVSOROLE=auth-role-vso-operator\n",
    "\n",
    "# Enable an instance of the Transit Secrets Engine.\n",
    "vault secrets enable -path=$TRANSITPATH transit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Create a secret cache configuration.  No. of entries.  Min is 10.  0 is unlimited.\n",
    "vault write $TRANSITPATH/cache-config size=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Create an encryption key\n",
    "vault write -force $TRANSITPATH/keys/$ENCKEYNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Create a policy for the operator role to access the encryption key\n",
    "vault policy write demo-auth-policy-operator - <<EOF\n",
    "path \"$TRANSITPATH/encrypt/$ENCKEYNAME\" {\n",
    "   capabilities = [\"create\", \"update\"]\n",
    "}\n",
    "path \"$TRANSITPATH/decrypt/$ENCKEYNAME\" {\n",
    "   capabilities = [\"create\", \"update\"]\n",
    "}\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Name of K8s VSO operator service account\n",
    "export KUBEVSOSVCACCOUNT=vso-operator\n",
    "\n",
    "# Create a new K8s Service Account for the operator.  We will be using the same namespace used by VSO.\n",
    "kubectl create -f - <<EOF\n",
    "apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  # SA bound to the VSO namespace for transit engine auth\n",
    "  namespace: vault-secrets-operator-system\n",
    "  name: $KUBEVSOSVCACCOUNT\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Create Kubernetes auth role for the K8s VSO operator service account\n",
    "# Note that the claim for audiences is set to \"vault\" and will be set similarly for the VaultAuth CRD below.\n",
    "\n",
    "# For this demo, we will be setting the K8s auth role's token period to 30s\n",
    "# This ensures the token has no max TTL but renewals is based on the token period\n",
    "# https://developer.hashicorp.com/vault/api-docs/auth/token#periodecho \"Vault K8s Auth Path: $K8SAUTHPATH\"\n",
    "echo \"Vault K8s VSO Role: $K8SVSOROLE\"\n",
    "echo \"K8s VSO Service Account: $KUBEVSOSVCACCOUNT\"\n",
    "vault write auth/$K8SAUTHPATH/role/$K8SVSOROLE \\\n",
    "   bound_service_account_names=$KUBEVSOSVCACCOUNT \\\n",
    "   bound_service_account_namespaces=vault-secrets-operator-system \\\n",
    "   token_policies=demo-auth-policy-operator \\\n",
    "   period=60 \\\n",
    "   audience=vault\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vault K8s Auth Path: kubernetes-minikube\n",
      "Vault K8s VSO Role: \n",
      "Vault Transit Engine Path: \n",
      "Vault Transit Engine Encryption Key: \n",
      "K8s VSO Service Account: \n",
      "The VaultAuth \"vault-auth-demo-transit\" is invalid: \n",
      "* spec.kubernetes.role: Required value\n",
      "* spec.kubernetes.serviceAccount: Required value\n",
      "* spec.storageEncryption.keyName: Required value\n",
      "* spec.storageEncryption.mount: Required value\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "# Create the VaultAuth CRD for the transit engine\n",
    "# You can view the CRD in k9s.  Type :crd and select 'vaultauths.secrets.hashicorp.com'.\n",
    "# You will this CRD in the vault-secrets-operator-system namespace\n",
    "echo \"Vault K8s Auth Path: $K8SAUTHPATH\"\n",
    "echo \"Vault K8s VSO Role: $K8SVSOROLE\"\n",
    "echo \"Vault Transit Engine Path: $TRANSITPATH\"\n",
    "echo \"Vault Transit Engine Encryption Key: $ENCKEYNAME\"\n",
    "echo \"K8s VSO Service Account: $KUBEVSOSVCACCOUNT\"\n",
    "\n",
    "kubectl create -f - <<EOF\n",
    "apiVersion: secrets.hashicorp.com/v1beta1\n",
    "kind: VaultAuth\n",
    "metadata:\n",
    "  name: vault-auth-demo-transit\n",
    "  # This Vault Auth is used for transit engine hence stays in the VSO namespace\n",
    "  namespace: vault-secrets-operator-system\n",
    "  labels:\n",
    "    cacheStorageEncryption: \"true\"\n",
    "spec:\n",
    "  method: kubernetes\n",
    "  mount: $K8SAUTHPATH\n",
    "  vaultConnectionRef: default\n",
    "  kubernetes:\n",
    "    role: $K8SVSOROLE\n",
    "    serviceAccount: $KUBEVSOSVCACCOUNT\n",
    "    audiences:\n",
    "      - vault\n",
    "  storageEncryption:\n",
    "    mount: $TRANSITPATH\n",
    "    keyName: $ENCKEYNAME\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Update the VSO helm chart to use caching\n",
    "# Note that this can also be done initially when installing the helm chart earlier\n",
    "tee patch.yaml <<EOF\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: manager\n",
    "        args:\n",
    "        - \"--client-cache-persistence-model=direct-encrypted\"\n",
    "EOF\n",
    "\n",
    "\n",
    "kubectl patch deployment vault-secrets-operator-controller-manager -n vault-secrets-operator-system --patch-file patch.yaml\n",
    "\n",
    "rm patch.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Stop and Start the VSO controller to turn on the caching\n",
    "kubectl scale deployment -n vault-secrets-operator-system vault-secrets-operator-controller-manager --replicas=0\n",
    "kubectl scale deployment -n vault-secrets-operator-system vault-secrets-operator-controller-manager --replicas=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# When the application pods restart, stop and start the VSO controller to see the impact of the cache\n",
    "kubectl scale deployment -n vault-secrets-operator-system vault-secrets-operator-controller-manager --replicas=0\n",
    "kubectl scale deployment -n vault-secrets-operator-system vault-secrets-operator-controller-manager --replicas=1\n",
    "\n",
    "# Use k9s to view the vault-secret-operator-controller-manager pod logs in the vault-secrets-operator-system namespace\n",
    "# You should see the clientCachePersistenceModel is set to \"direct-encrypted\"\n",
    "# You should also be able to see the DB lease renewal happening.\n",
    "# This shows that the VSO cache configuration is configured properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T08:21:00.046755Z",
     "start_time": "2024-10-11T08:20:50.836818Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "# Disable the database secrets engine\n",
    "vault secrets disable database\n",
    "\n",
    "# stop vault\n",
    "docker stop vault-enterprise\n",
    "\n",
    "# stop postgres\n",
    "docker stop postgres\n",
    "\n",
    "# Uninstall VSO helm chart\n",
    "helm uninstall vault-secrets-operator -n vault-secrets-operator-system \n",
    "# stop minikube\n",
    "minikube delete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix - Other Useful Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# For debugging, for testing the K8s API\n",
    "echo \"JWT Token: $JWT_TOKEN_DEFAULT_DEMONS\"\n",
    "echo \"K8s API Internal URL: $KUBE_INT_API\"\n",
    "curl $KUBE_INT_API/apis/apps/v1/ \\\n",
    "  --cacert ~/.minikube/ca.crt  \\\n",
    "  --header \"Authorization: Bearer $JWT_TOKEN_DEFAULT_DEMONS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# For testing connectivity.  Bash shell to a temporary pod and do curl commands\n",
    "kubectl run my-shell --rm -i --tty --image ubuntu -- bash\n",
    "apt update\n",
    "apt install curl\n",
    "curl http://host.minikube.internal:8200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# View all K8s pod details - wide format\n",
    "kubectl get pod -o wide -A\n",
    "\n",
    "# Getting K8s secrets sample\n",
    "kubectl get secrets -n $KUBENAMESPACE  --output=json | jq -r '.items[].metadata | select(.name|startswith(\"vault-token-\")).name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Delete previous deployment example\n",
    "kubectl delete deployment -n $KUBENAMESPACE vso-db-demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Docker network commands\n",
    "# Internal host name - host.docker.internal\n",
    "docker network ls\n",
    "docker network connect <network> <container>\n",
    "docker network create --driver bridge vso-connect\n",
    "\n",
    "# Get IP address of Vault pod\n",
    "docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' vault-enterprise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Audit <==\n",
      "|---------|-------------------|----------|-------|---------|---------------------|---------------------|\n",
      "| Command |       Args        | Profile  | User  | Version |     Start Time      |      End Time       |\n",
      "|---------|-------------------|----------|-------|---------|---------------------|---------------------|\n",
      "| delete  |                   | minikube | kz.li | v1.33.1 | 24 Jun 24 12:19 +08 | 24 Jun 24 12:20 +08 |\n",
      "| start   |                   | minikube | kz.li | v1.33.1 | 24 Jun 24 12:20 +08 | 24 Jun 24 12:20 +08 |\n",
      "| delete  |                   | minikube | kz.li | v1.33.1 | 24 Jun 24 12:34 +08 | 24 Jun 24 12:34 +08 |\n",
      "| start   |                   | minikube | kz.li | v1.33.1 | 24 Jun 24 12:34 +08 | 24 Jun 24 12:34 +08 |\n",
      "| delete  |                   | minikube | kz.li | v1.33.1 | 24 Jun 24 12:54 +08 | 24 Jun 24 12:54 +08 |\n",
      "| image   | load nginx:latest | minikube | kz.li | v1.33.1 | 25 Jun 24 10:18 +08 |                     |\n",
      "| start   |                   | minikube | kz.li | v1.33.1 | 25 Jun 24 14:59 +08 | 25 Jun 24 15:00 +08 |\n",
      "| delete  |                   | minikube | kz.li | v1.33.1 | 26 Jun 24 09:54 +08 | 26 Jun 24 09:54 +08 |\n",
      "| start   |                   | minikube | kz.li | v1.33.1 | 02 Jul 24 15:38 +08 | 02 Jul 24 15:39 +08 |\n",
      "| image   | load nginx:latest | minikube | kz.li | v1.33.1 | 02 Jul 24 15:39 +08 | 02 Jul 24 15:39 +08 |\n",
      "| delete  |                   | minikube | kz.li | v1.33.1 | 02 Jul 24 15:54 +08 | 02 Jul 24 15:54 +08 |\n",
      "| delete  |                   | minikube | kz.li | v1.33.1 | 07 Oct 24 17:49 +08 | 07 Oct 24 17:49 +08 |\n",
      "| start   |                   | minikube | kz.li | v1.33.1 | 08 Oct 24 11:46 +08 | 08 Oct 24 11:46 +08 |\n",
      "| image   | load nginx:latest | minikube | kz.li | v1.33.1 | 08 Oct 24 11:46 +08 | 08 Oct 24 11:47 +08 |\n",
      "| delete  |                   | minikube | kz.li | v1.33.1 | 08 Oct 24 11:53 +08 | 08 Oct 24 11:54 +08 |\n",
      "| start   |                   | minikube | kz.li | v1.33.1 | 08 Oct 24 11:54 +08 | 08 Oct 24 11:54 +08 |\n",
      "| image   | load nginx:latest | minikube | kz.li | v1.33.1 | 08 Oct 24 11:54 +08 | 08 Oct 24 11:54 +08 |\n",
      "| delete  |                   | minikube | kz.li | v1.33.1 | 08 Oct 24 14:24 +08 | 08 Oct 24 14:24 +08 |\n",
      "| start   |                   | minikube | kz.li | v1.33.1 | 08 Oct 24 14:24 +08 | 08 Oct 24 14:24 +08 |\n",
      "| delete  |                   | minikube | kz.li | v1.33.1 | 08 Oct 24 14:35 +08 | 08 Oct 24 14:35 +08 |\n",
      "| start   |                   | minikube | kz.li | v1.34.0 | 08 Oct 24 14:51 +08 | 08 Oct 24 14:52 +08 |\n",
      "| delete  |                   | minikube | kz.li | v1.34.0 | 08 Oct 24 15:05 +08 | 08 Oct 24 15:05 +08 |\n",
      "| start   |                   | minikube | kz.li | v1.34.0 | 08 Oct 24 15:05 +08 | 08 Oct 24 15:05 +08 |\n",
      "| image   | load nginx:latest | minikube | kz.li | v1.34.0 | 08 Oct 24 15:06 +08 | 08 Oct 24 15:06 +08 |\n",
      "| delete  |                   | minikube | kz.li | v1.34.0 | 08 Oct 24 15:37 +08 | 08 Oct 24 15:37 +08 |\n",
      "| start   |                   | minikube | kz.li | v1.34.0 | 08 Oct 24 15:39 +08 | 08 Oct 24 15:39 +08 |\n",
      "| delete  |                   | minikube | kz.li | v1.34.0 | 08 Oct 24 16:14 +08 | 08 Oct 24 16:14 +08 |\n",
      "| delete  |                   | minikube | kz.li | v1.34.0 | 08 Oct 24 17:43 +08 | 08 Oct 24 17:43 +08 |\n",
      "| start   |                   | minikube | kz.li | v1.34.0 | 08 Oct 24 17:43 +08 | 08 Oct 24 17:44 +08 |\n",
      "| delete  |                   | minikube | kz.li | v1.34.0 | 08 Oct 24 17:47 +08 | 08 Oct 24 17:47 +08 |\n",
      "| start   |                   | minikube | kz.li | v1.34.0 | 10 Oct 24 13:46 +08 | 10 Oct 24 13:46 +08 |\n",
      "| delete  |                   | minikube | kz.li | v1.34.0 | 10 Oct 24 13:55 +08 | 10 Oct 24 13:55 +08 |\n",
      "| start   |                   | minikube | kz.li | v1.34.0 | 11 Oct 24 10:41 +08 | 11 Oct 24 10:41 +08 |\n",
      "| delete  |                   | minikube | kz.li | v1.34.0 | 11 Oct 24 16:20 +08 | 11 Oct 24 16:20 +08 |\n",
      "| start   |                   | minikube | kz.li | v1.34.0 | 30 Oct 24 08:53 +08 | 30 Oct 24 08:54 +08 |\n",
      "| start   |                   | minikube | kz.li | v1.34.0 | 12 Nov 24 10:31 +08 | 12 Nov 24 10:32 +08 |\n",
      "| image   | load nginx:latest | minikube | kz.li | v1.34.0 | 12 Nov 24 10:32 +08 | 12 Nov 24 10:32 +08 |\n",
      "| delete  |                   | minikube | kz.li | v1.34.0 | 12 Nov 24 10:37 +08 | 12 Nov 24 10:37 +08 |\n",
      "| start   |                   | minikube | kz.li | v1.34.0 | 14 Nov 24 10:48 +08 | 14 Nov 24 10:48 +08 |\n",
      "| delete  |                   | minikube | kz.li | v1.34.0 | 14 Nov 24 11:42 +08 | 14 Nov 24 11:42 +08 |\n",
      "| start   |                   | minikube | kz.li | v1.34.0 | 10 Dec 24 17:02 +08 | 10 Dec 24 17:02 +08 |\n",
      "| stop    |                   | minikube | kz.li | v1.34.0 | 10 Dec 24 17:05 +08 | 10 Dec 24 17:06 +08 |\n",
      "| start   |                   | minikube | kz.li | v1.35.0 | 13 Feb 25 10:04 +08 | 13 Feb 25 10:04 +08 |\n",
      "| image   | load nginx:latest | minikube | kz.li | v1.35.0 | 13 Feb 25 10:04 +08 |                     |\n",
      "| delete  |                   | minikube | kz.li | v1.35.0 | 13 Feb 25 10:14 +08 | 13 Feb 25 10:14 +08 |\n",
      "| start   |                   | minikube | kz.li | v1.35.0 | 13 Feb 25 10:15 +08 | 13 Feb 25 10:15 +08 |\n",
      "| image   | load nginx:latest | minikube | kz.li | v1.35.0 | 13 Feb 25 10:16 +08 | 13 Feb 25 10:16 +08 |\n",
      "| delete  |                   | minikube | kz.li | v1.35.0 | 13 Feb 25 10:52 +08 | 13 Feb 25 10:52 +08 |\n",
      "| start   |                   | minikube | kz.li | v1.35.0 | 14 Feb 25 11:48 +08 | 14 Feb 25 11:48 +08 |\n",
      "| delete  |                   | minikube | kz.li | v1.35.0 | 14 Feb 25 12:08 +08 | 14 Feb 25 12:08 +08 |\n",
      "| start   |                   | minikube | kz.li | v1.35.0 | 14 Feb 25 14:23 +08 | 14 Feb 25 14:23 +08 |\n",
      "| image   | load nginx:latest | minikube | kz.li | v1.35.0 | 14 Feb 25 14:26 +08 | 14 Feb 25 14:27 +08 |\n",
      "| start   |                   | minikube | kz.li | v1.35.0 | 18 Mar 25 11:54 +08 |                     |\n",
      "| start   |                   | minikube | kz.li | v1.35.0 | 18 Mar 25 12:03 +08 |                     |\n",
      "| start   |                   | minikube | kz.li | v1.35.0 | 18 Mar 25 14:29 +08 |                     |\n",
      "| start   |                   | minikube | kz.li | v1.35.0 | 18 Mar 25 14:36 +08 |                     |\n",
      "| start   |                   | minikube | kz.li | v1.35.0 | 18 Mar 25 14:41 +08 |                     |\n",
      "| image   | load nginx:latest | minikube | kz.li | v1.35.0 | 18 Mar 25 14:46 +08 | 18 Mar 25 14:47 +08 |\n",
      "| delete  |                   | minikube | kz.li | v1.35.0 | 18 Mar 25 14:52 +08 | 18 Mar 25 14:52 +08 |\n",
      "| start   |                   | minikube | kz.li | v1.35.0 | 18 Mar 25 14:53 +08 | 18 Mar 25 14:53 +08 |\n",
      "|---------|-------------------|----------|-------|---------|---------------------|---------------------|\n",
      "\n",
      "\n",
      "==> Last Start <==\n",
      "Log file created at: 2025/03/18 14:53:08\n",
      "Running on machine: kz\n",
      "Binary: Built with gc go1.23.4 for darwin/arm64\n",
      "Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\n",
      "I0318 14:53:08.139162    2593 out.go:345] Setting OutFile to fd 1 ...\n",
      "I0318 14:53:08.139330    2593 out.go:392] TERM=,COLORTERM=, which probably does not support color\n",
      "I0318 14:53:08.139332    2593 out.go:358] Setting ErrFile to fd 2...\n",
      "I0318 14:53:08.139333    2593 out.go:392] TERM=,COLORTERM=, which probably does not support color\n",
      "I0318 14:53:08.139445    2593 root.go:338] Updating PATH: /Users/kz.li/.minikube/bin\n",
      "I0318 14:53:08.140625    2593 out.go:352] Setting JSON to false\n",
      "I0318 14:53:08.170205    2593 start.go:129] hostinfo: {\"hostname\":\"kz.li-YF071QVQ4V\",\"uptime\":797,\"bootTime\":1742279991,\"procs\":609,\"os\":\"darwin\",\"platform\":\"darwin\",\"platformFamily\":\"Standalone Workstation\",\"platformVersion\":\"14.7.1\",\"kernelVersion\":\"23.6.0\",\"kernelArch\":\"arm64\",\"virtualizationSystem\":\"\",\"virtualizationRole\":\"\",\"hostId\":\"6aaa5b1b-d13b-51a1-933b-cd3b3a2f8f0f\"}\n",
      "W0318 14:53:08.170295    2593 start.go:137] gopshost.Virtualization returned error: not implemented yet\n",
      "I0318 14:53:08.175754    2593 out.go:177] * minikube v1.35.0 on Darwin 14.7.1 (arm64)\n",
      "I0318 14:53:08.181791    2593 notify.go:220] Checking for updates...\n",
      "I0318 14:53:08.182038    2593 driver.go:394] Setting default libvirt URI to qemu:///system\n",
      "I0318 14:53:08.182066    2593 global.go:112] Querying for installed drivers using PATH=/Users/kz.li/.minikube/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin\n",
      "I0318 14:53:08.182185    2593 global.go:133] qemu2 default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: \"qemu-system-aarch64\": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}\n",
      "I0318 14:53:08.182235    2593 global.go:133] vfkit default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: \"vfkit\": executable file not found in $PATH Reason: Fix:Run 'brew tap cfergeau/crc && brew install vfkit' Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vfkit/ Version:}\n",
      "I0318 14:53:08.182301    2593 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}\n",
      "I0318 14:53:08.182329    2593 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: \"vmrun\": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}\n",
      "I0318 14:53:08.205407    2593 docker.go:123] docker version: linux-27.4.0:Docker Desktop 4.37.2 (179585)\n",
      "I0318 14:53:08.205564    2593 cli_runner.go:164] Run: docker system info --format \"{{json .}}\"\n",
      "I0318 14:53:08.433365    2593 info.go:266] docker info: {ID:861994c9-a997-4860-b14a-52fed3bee97e Containers:6 ContainersRunning:2 ContainersPaused:0 ContainersStopped:4 Images:44 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:63 OomKillDisable:false NGoroutines:84 SystemTime:2025-03-18 06:53:08.423651178 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:6.10.14-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:8217686016 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/kz.li/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:27.4.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:472731909fa34bd7bc9c087e4c27943f9835f111 Expected:472731909fa34bd7bc9c087e4c27943f9835f111} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/Users/kz.li/.docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.5.1] map[Name:buildx Path:/Users/kz.li/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.19.2-desktop.1] map[Name:compose Path:/Users/kz.li/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.31.0-desktop.2] map[Name:debug Path:/Users/kz.li/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.37] map[Name:desktop Path:/Users/kz.li/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.0] map[Name:dev Path:/Users/kz.li/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/kz.li/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:/Users/kz.li/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/Users/kz.li/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:/Users/kz.li/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/kz.li/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.15.1]] Warnings:<nil>}}\n",
      "I0318 14:53:08.433451    2593 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}\n",
      "I0318 14:53:08.433466    2593 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}\n",
      "I0318 14:53:08.433564    2593 global.go:133] hyperkit default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: \"hyperkit\": executable file not found in $PATH Reason: Fix:Run 'brew install hyperkit' Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/hyperkit/ Version:}\n",
      "I0318 14:53:08.433601    2593 global.go:133] parallels default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: \"prlctl\": executable file not found in $PATH Reason: Fix:Install Parallels Desktop for Mac Doc:https://minikube.sigs.k8s.io/docs/drivers/parallels/ Version:}\n",
      "I0318 14:53:08.433648    2593 global.go:133] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: \"podman\": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}\n",
      "I0318 14:53:08.433662    2593 driver.go:316] not recommending \"ssh\" due to default: false\n",
      "I0318 14:53:08.433674    2593 driver.go:351] Picked: docker\n",
      "I0318 14:53:08.433677    2593 driver.go:352] Alternatives: [ssh]\n",
      "I0318 14:53:08.433679    2593 driver.go:353] Rejects: [qemu2 vfkit virtualbox vmware hyperkit parallels podman]\n",
      "I0318 14:53:08.439718    2593 out.go:177] * Automatically selected the docker driver\n",
      "I0318 14:53:08.442783    2593 start.go:297] selected driver: docker\n",
      "I0318 14:53:08.442788    2593 start.go:901] validating driver \"docker\" against <nil>\n",
      "I0318 14:53:08.442796    2593 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}\n",
      "I0318 14:53:08.442898    2593 cli_runner.go:164] Run: docker system info --format \"{{json .}}\"\n",
      "I0318 14:53:08.508969    2593 info.go:266] docker info: {ID:861994c9-a997-4860-b14a-52fed3bee97e Containers:6 ContainersRunning:2 ContainersPaused:0 ContainersStopped:4 Images:44 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:63 OomKillDisable:false NGoroutines:84 SystemTime:2025-03-18 06:53:08.502213886 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:6.10.14-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:8217686016 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/kz.li/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:27.4.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:472731909fa34bd7bc9c087e4c27943f9835f111 Expected:472731909fa34bd7bc9c087e4c27943f9835f111} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/Users/kz.li/.docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.5.1] map[Name:buildx Path:/Users/kz.li/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.19.2-desktop.1] map[Name:compose Path:/Users/kz.li/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.31.0-desktop.2] map[Name:debug Path:/Users/kz.li/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.37] map[Name:desktop Path:/Users/kz.li/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.0] map[Name:dev Path:/Users/kz.li/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/kz.li/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:/Users/kz.li/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/Users/kz.li/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:/Users/kz.li/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/kz.li/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.15.1]] Warnings:<nil>}}\n",
      "I0318 14:53:08.509105    2593 start_flags.go:310] no existing cluster config was found, will generate one from the flags \n",
      "I0318 14:53:08.509178    2593 start_flags.go:393] Using suggested 7788MB memory alloc based on sys=36864MB, container=7836MB\n",
      "I0318 14:53:08.510040    2593 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]\n",
      "I0318 14:53:08.513667    2593 out.go:177] * Using Docker Desktop driver with root privileges\n",
      "I0318 14:53:08.518597    2593 cni.go:84] Creating CNI manager for \"\"\n",
      "I0318 14:53:08.518624    2593 cni.go:158] \"docker\" driver + \"docker\" container runtime found on kubernetes v1.24+, recommending bridge\n",
      "I0318 14:53:08.518634    2593 start_flags.go:319] Found \"bridge CNI\" CNI - setting NetworkPlugin=cni\n",
      "I0318 14:53:08.518673    2593 start.go:340] cluster config:\n",
      "{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:7788 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}\n",
      "I0318 14:53:08.525630    2593 out.go:177] * Starting \"minikube\" primary control-plane node in \"minikube\" cluster\n",
      "I0318 14:53:08.530734    2593 cache.go:121] Beginning downloading kic base image for docker with docker\n",
      "I0318 14:53:08.532230    2593 out.go:177] * Pulling base image v0.0.46 ...\n",
      "I0318 14:53:08.537633    2593 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker\n",
      "I0318 14:53:08.537649    2593 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon\n",
      "I0318 14:53:08.537663    2593 preload.go:146] Found local preload: /Users/kz.li/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-arm64.tar.lz4\n",
      "I0318 14:53:08.537668    2593 cache.go:56] Caching tarball of preloaded images\n",
      "I0318 14:53:08.537760    2593 preload.go:172] Found /Users/kz.li/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-arm64.tar.lz4 in cache, skipping download\n",
      "I0318 14:53:08.537767    2593 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker\n",
      "I0318 14:53:08.538359    2593 profile.go:143] Saving config to /Users/kz.li/.minikube/profiles/minikube/config.json ...\n",
      "I0318 14:53:08.538393    2593 lock.go:35] WriteFile acquiring /Users/kz.li/.minikube/profiles/minikube/config.json: {Name:mkdc77789779f71bae16a482d9273e426749f5bc Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}\n",
      "I0318 14:53:08.571841    2593 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon, skipping pull\n",
      "I0318 14:53:08.571848    2593 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in daemon, skipping load\n",
      "I0318 14:53:08.571856    2593 cache.go:227] Successfully downloaded all kic artifacts\n",
      "I0318 14:53:08.571880    2593 start.go:360] acquireMachinesLock for minikube: {Name:mkab13f291bcfaaa2e7282a56578584f047058a1 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}\n",
      "I0318 14:53:08.571970    2593 start.go:364] duration metric: took 84.5µs to acquireMachinesLock for \"minikube\"\n",
      "I0318 14:53:08.571978    2593 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:7788 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}\n",
      "I0318 14:53:08.572014    2593 start.go:125] createHost starting for \"\" (driver=\"docker\")\n",
      "I0318 14:53:08.576711    2593 out.go:235] * Creating docker container (CPUs=2, Memory=7788MB) ...\n",
      "I0318 14:53:08.576806    2593 start.go:159] libmachine.API.Create for \"minikube\" (driver=\"docker\")\n",
      "I0318 14:53:08.576816    2593 client.go:168] LocalClient.Create starting\n",
      "I0318 14:53:08.577127    2593 main.go:141] libmachine: Reading certificate data from /Users/kz.li/.minikube/certs/ca.pem\n",
      "I0318 14:53:08.577278    2593 main.go:141] libmachine: Decoding PEM data...\n",
      "I0318 14:53:08.577285    2593 main.go:141] libmachine: Parsing certificate...\n",
      "I0318 14:53:08.577325    2593 main.go:141] libmachine: Reading certificate data from /Users/kz.li/.minikube/certs/cert.pem\n",
      "I0318 14:53:08.577414    2593 main.go:141] libmachine: Decoding PEM data...\n",
      "I0318 14:53:08.577418    2593 main.go:141] libmachine: Parsing certificate...\n",
      "I0318 14:53:08.577769    2593 cli_runner.go:164] Run: docker network inspect minikube --format \"{\"Name\": \"{{.Name}}\",\"Driver\": \"{{.Driver}}\",\"Subnet\": \"{{range .IPAM.Config}}{{.Subnet}}{{end}}\",\"Gateway\": \"{{range .IPAM.Config}}{{.Gateway}}{{end}}\",\"MTU\": {{if (index .Options \"com.docker.network.driver.mtu\")}}{{(index .Options \"com.docker.network.driver.mtu\")}}{{else}}0{{end}}, \"ContainerIPs\": [{{range $k,$v := .Containers }}\"{{$v.IPv4Address}}\",{{end}}]}\"\n",
      "W0318 14:53:08.591493    2593 cli_runner.go:211] docker network inspect minikube --format \"{\"Name\": \"{{.Name}}\",\"Driver\": \"{{.Driver}}\",\"Subnet\": \"{{range .IPAM.Config}}{{.Subnet}}{{end}}\",\"Gateway\": \"{{range .IPAM.Config}}{{.Gateway}}{{end}}\",\"MTU\": {{if (index .Options \"com.docker.network.driver.mtu\")}}{{(index .Options \"com.docker.network.driver.mtu\")}}{{else}}0{{end}}, \"ContainerIPs\": [{{range $k,$v := .Containers }}\"{{$v.IPv4Address}}\",{{end}}]}\" returned with exit code 1\n",
      "I0318 14:53:08.591559    2593 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...\n",
      "I0318 14:53:08.591565    2593 cli_runner.go:164] Run: docker network inspect minikube\n",
      "W0318 14:53:08.605150    2593 cli_runner.go:211] docker network inspect minikube returned with exit code 1\n",
      "I0318 14:53:08.605162    2593 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1\n",
      "stdout:\n",
      "[]\n",
      "\n",
      "stderr:\n",
      "Error response from daemon: network minikube not found\n",
      "I0318 14:53:08.605169    2593 network_create.go:289] output of [docker network inspect minikube]: -- stdout --\n",
      "[]\n",
      "\n",
      "-- /stdout --\n",
      "** stderr ** \n",
      "Error response from daemon: network minikube not found\n",
      "\n",
      "** /stderr **\n",
      "I0318 14:53:08.605270    2593 cli_runner.go:164] Run: docker network inspect bridge --format \"{\"Name\": \"{{.Name}}\",\"Driver\": \"{{.Driver}}\",\"Subnet\": \"{{range .IPAM.Config}}{{.Subnet}}{{end}}\",\"Gateway\": \"{{range .IPAM.Config}}{{.Gateway}}{{end}}\",\"MTU\": {{if (index .Options \"com.docker.network.driver.mtu\")}}{{(index .Options \"com.docker.network.driver.mtu\")}}{{else}}0{{end}}, \"ContainerIPs\": [{{range $k,$v := .Containers }}\"{{$v.IPv4Address}}\",{{end}}]}\"\n",
      "I0318 14:53:08.619726    2593 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0x140016d8df0}\n",
      "I0318 14:53:08.619746    2593 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 65535 ...\n",
      "I0318 14:53:08.619789    2593 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=65535 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube\n",
      "I0318 14:53:08.659727    2593 network_create.go:108] docker network minikube 192.168.49.0/24 created\n",
      "I0318 14:53:08.659840    2593 kic.go:121] calculated static IP \"192.168.49.2\" for the \"minikube\" container\n",
      "I0318 14:53:08.660031    2593 cli_runner.go:164] Run: docker ps -a --format {{.Names}}\n",
      "I0318 14:53:08.674318    2593 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true\n",
      "I0318 14:53:08.690317    2593 oci.go:103] Successfully created a docker volume minikube\n",
      "I0318 14:53:08.690414    2593 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -d /var/lib\n",
      "I0318 14:53:08.986133    2593 oci.go:107] Successfully prepared a docker volume minikube\n",
      "I0318 14:53:08.986175    2593 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker\n",
      "I0318 14:53:08.986189    2593 kic.go:194] Starting extracting preloaded images to volume ...\n",
      "I0318 14:53:08.986327    2593 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /Users/kz.li/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir\n",
      "I0318 14:53:10.180229    2593 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /Users/kz.li/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir: (1.193840416s)\n",
      "I0318 14:53:10.180251    2593 kic.go:203] duration metric: took 1.194064208s to extract preloaded images to volume ...\n",
      "I0318 14:53:10.180398    2593 cli_runner.go:164] Run: docker info --format \"'{{json .SecurityOptions}}'\"\n",
      "I0318 14:53:10.258612    2593 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=7788mb --memory-swap=7788mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279\n",
      "I0318 14:53:10.394194    2593 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}\n",
      "I0318 14:53:10.408758    2593 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}\n",
      "I0318 14:53:10.427536    2593 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables\n",
      "I0318 14:53:10.480028    2593 oci.go:144] the created container \"minikube\" has a running status.\n",
      "I0318 14:53:10.480057    2593 kic.go:225] Creating ssh key for kic: /Users/kz.li/.minikube/machines/minikube/id_rsa...\n",
      "I0318 14:53:10.533728    2593 kic_runner.go:191] docker (temp): /Users/kz.li/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)\n",
      "I0318 14:53:10.557535    2593 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}\n",
      "I0318 14:53:10.572818    2593 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys\n",
      "I0318 14:53:10.572834    2593 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]\n",
      "I0318 14:53:10.614080    2593 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}\n",
      "I0318 14:53:10.627827    2593 machine.go:93] provisionDockerMachine start ...\n",
      "I0318 14:53:10.627933    2593 cli_runner.go:164] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"22/tcp\") 0).HostPort}}'\" minikube\n",
      "I0318 14:53:10.642321    2593 main.go:141] libmachine: Using SSH client type: native\n",
      "I0318 14:53:10.642484    2593 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050b8790] 0x1050bafd0 <nil>  [] 0s} 127.0.0.1 52320 <nil> <nil>}\n",
      "I0318 14:53:10.642487    2593 main.go:141] libmachine: About to run SSH command:\n",
      "hostname\n",
      "I0318 14:53:10.741492    2593 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube\n",
      "\n",
      "I0318 14:53:10.741507    2593 ubuntu.go:169] provisioning hostname \"minikube\"\n",
      "I0318 14:53:10.741593    2593 cli_runner.go:164] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"22/tcp\") 0).HostPort}}'\" minikube\n",
      "I0318 14:53:10.757873    2593 main.go:141] libmachine: Using SSH client type: native\n",
      "I0318 14:53:10.758012    2593 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050b8790] 0x1050bafd0 <nil>  [] 0s} 127.0.0.1 52320 <nil> <nil>}\n",
      "I0318 14:53:10.758015    2593 main.go:141] libmachine: About to run SSH command:\n",
      "sudo hostname minikube && echo \"minikube\" | sudo tee /etc/hostname\n",
      "I0318 14:53:10.867551    2593 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube\n",
      "\n",
      "I0318 14:53:10.867713    2593 cli_runner.go:164] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"22/tcp\") 0).HostPort}}'\" minikube\n",
      "I0318 14:53:10.894459    2593 main.go:141] libmachine: Using SSH client type: native\n",
      "I0318 14:53:10.894685    2593 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050b8790] 0x1050bafd0 <nil>  [] 0s} 127.0.0.1 52320 <nil> <nil>}\n",
      "I0318 14:53:10.894696    2593 main.go:141] libmachine: About to run SSH command:\n",
      "\n",
      "\t\tif ! grep -xq '.*\\sminikube' /etc/hosts; then\n",
      "\t\t\tif grep -xq '127.0.1.1\\s.*' /etc/hosts; then\n",
      "\t\t\t\tsudo sed -i 's/^127.0.1.1\\s.*/127.0.1.1 minikube/g' /etc/hosts;\n",
      "\t\t\telse \n",
      "\t\t\t\techo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; \n",
      "\t\t\tfi\n",
      "\t\tfi\n",
      "I0318 14:53:11.003957    2593 main.go:141] libmachine: SSH cmd err, output: <nil>: \n",
      "I0318 14:53:11.004011    2593 ubuntu.go:175] set auth options {CertDir:/Users/kz.li/.minikube CaCertPath:/Users/kz.li/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/kz.li/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/kz.li/.minikube/machines/server.pem ServerKeyPath:/Users/kz.li/.minikube/machines/server-key.pem ClientKeyPath:/Users/kz.li/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/kz.li/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/kz.li/.minikube}\n",
      "I0318 14:53:11.004068    2593 ubuntu.go:177] setting up certificates\n",
      "I0318 14:53:11.004088    2593 provision.go:84] configureAuth start\n",
      "I0318 14:53:11.004382    2593 cli_runner.go:164] Run: docker container inspect -f \"{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}\" minikube\n",
      "I0318 14:53:11.035190    2593 provision.go:143] copyHostCerts\n",
      "I0318 14:53:11.035320    2593 exec_runner.go:144] found /Users/kz.li/.minikube/ca.pem, removing ...\n",
      "I0318 14:53:11.035328    2593 exec_runner.go:203] rm: /Users/kz.li/.minikube/ca.pem\n",
      "I0318 14:53:11.036008    2593 exec_runner.go:151] cp: /Users/kz.li/.minikube/certs/ca.pem --> /Users/kz.li/.minikube/ca.pem (1074 bytes)\n",
      "I0318 14:53:11.036276    2593 exec_runner.go:144] found /Users/kz.li/.minikube/cert.pem, removing ...\n",
      "I0318 14:53:11.036278    2593 exec_runner.go:203] rm: /Users/kz.li/.minikube/cert.pem\n",
      "I0318 14:53:11.036736    2593 exec_runner.go:151] cp: /Users/kz.li/.minikube/certs/cert.pem --> /Users/kz.li/.minikube/cert.pem (1119 bytes)\n",
      "I0318 14:53:11.037207    2593 exec_runner.go:144] found /Users/kz.li/.minikube/key.pem, removing ...\n",
      "I0318 14:53:11.037210    2593 exec_runner.go:203] rm: /Users/kz.li/.minikube/key.pem\n",
      "I0318 14:53:11.037712    2593 exec_runner.go:151] cp: /Users/kz.li/.minikube/certs/key.pem --> /Users/kz.li/.minikube/key.pem (1675 bytes)\n",
      "I0318 14:53:11.038138    2593 provision.go:117] generating server cert: /Users/kz.li/.minikube/machines/server.pem ca-key=/Users/kz.li/.minikube/certs/ca.pem private-key=/Users/kz.li/.minikube/certs/ca-key.pem org=kz.li.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]\n",
      "I0318 14:53:11.149875    2593 provision.go:177] copyRemoteCerts\n",
      "I0318 14:53:11.149996    2593 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker\n",
      "I0318 14:53:11.150043    2593 cli_runner.go:164] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"22/tcp\") 0).HostPort}}'\" minikube\n",
      "I0318 14:53:11.168634    2593 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52320 SSHKeyPath:/Users/kz.li/.minikube/machines/minikube/id_rsa Username:docker}\n",
      "I0318 14:53:11.261528    2593 ssh_runner.go:362] scp /Users/kz.li/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)\n",
      "I0318 14:53:11.278423    2593 ssh_runner.go:362] scp /Users/kz.li/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)\n",
      "I0318 14:53:11.290539    2593 ssh_runner.go:362] scp /Users/kz.li/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)\n",
      "I0318 14:53:11.301285    2593 provision.go:87] duration metric: took 297.183208ms to configureAuth\n",
      "I0318 14:53:11.301295    2593 ubuntu.go:193] setting minikube options for container-runtime\n",
      "I0318 14:53:11.301458    2593 config.go:182] Loaded profile config \"minikube\": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0\n",
      "I0318 14:53:11.301522    2593 cli_runner.go:164] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"22/tcp\") 0).HostPort}}'\" minikube\n",
      "I0318 14:53:11.320847    2593 main.go:141] libmachine: Using SSH client type: native\n",
      "I0318 14:53:11.321035    2593 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050b8790] 0x1050bafd0 <nil>  [] 0s} 127.0.0.1 52320 <nil> <nil>}\n",
      "I0318 14:53:11.321038    2593 main.go:141] libmachine: About to run SSH command:\n",
      "df --output=fstype / | tail -n 1\n",
      "I0318 14:53:11.427610    2593 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay\n",
      "\n",
      "I0318 14:53:11.427634    2593 ubuntu.go:71] root file system type: overlay\n",
      "I0318 14:53:11.427780    2593 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...\n",
      "I0318 14:53:11.427930    2593 cli_runner.go:164] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"22/tcp\") 0).HostPort}}'\" minikube\n",
      "I0318 14:53:11.451344    2593 main.go:141] libmachine: Using SSH client type: native\n",
      "I0318 14:53:11.451543    2593 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050b8790] 0x1050bafd0 <nil>  [] 0s} 127.0.0.1 52320 <nil> <nil>}\n",
      "I0318 14:53:11.451586    2593 main.go:141] libmachine: About to run SSH command:\n",
      "sudo mkdir -p /lib/systemd/system && printf %s \"[Unit]\n",
      "Description=Docker Application Container Engine\n",
      "Documentation=https://docs.docker.com\n",
      "BindsTo=containerd.service\n",
      "After=network-online.target firewalld.service containerd.service\n",
      "Wants=network-online.target\n",
      "Requires=docker.socket\n",
      "StartLimitBurst=3\n",
      "StartLimitIntervalSec=60\n",
      "\n",
      "[Service]\n",
      "Type=notify\n",
      "Restart=on-failure\n",
      "\n",
      "\n",
      "\n",
      "# This file is a systemd drop-in unit that inherits from the base dockerd configuration.\n",
      "# The base configuration already specifies an 'ExecStart=...' command. The first directive\n",
      "# here is to clear out that command inherited from the base configuration. Without this,\n",
      "# the command from the base configuration and the command specified here are treated as\n",
      "# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd\n",
      "# will catch this invalid input and refuse to start the service with an error like:\n",
      "#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.\n",
      "\n",
      "# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other\n",
      "# container runtimes. If left unlimited, it may result in OOM issues with MySQL.\n",
      "ExecStart=\n",
      "ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 \n",
      "ExecReload=/bin/kill -s HUP \\$MAINPID\n",
      "\n",
      "# Having non-zero Limit*s causes performance problems due to accounting overhead\n",
      "# in the kernel. We recommend using cgroups to do container-local accounting.\n",
      "LimitNOFILE=infinity\n",
      "LimitNPROC=infinity\n",
      "LimitCORE=infinity\n",
      "\n",
      "# Uncomment TasksMax if your systemd version supports it.\n",
      "# Only systemd 226 and above support this version.\n",
      "TasksMax=infinity\n",
      "TimeoutStartSec=0\n",
      "\n",
      "# set delegate yes so that systemd does not reset the cgroups of docker containers\n",
      "Delegate=yes\n",
      "\n",
      "# kill only the docker process, not all processes in the cgroup\n",
      "KillMode=process\n",
      "\n",
      "[Install]\n",
      "WantedBy=multi-user.target\n",
      "\" | sudo tee /lib/systemd/system/docker.service.new\n",
      "I0318 14:53:11.572538    2593 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]\n",
      "Description=Docker Application Container Engine\n",
      "Documentation=https://docs.docker.com\n",
      "BindsTo=containerd.service\n",
      "After=network-online.target firewalld.service containerd.service\n",
      "Wants=network-online.target\n",
      "Requires=docker.socket\n",
      "StartLimitBurst=3\n",
      "StartLimitIntervalSec=60\n",
      "\n",
      "[Service]\n",
      "Type=notify\n",
      "Restart=on-failure\n",
      "\n",
      "\n",
      "\n",
      "# This file is a systemd drop-in unit that inherits from the base dockerd configuration.\n",
      "# The base configuration already specifies an 'ExecStart=...' command. The first directive\n",
      "# here is to clear out that command inherited from the base configuration. Without this,\n",
      "# the command from the base configuration and the command specified here are treated as\n",
      "# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd\n",
      "# will catch this invalid input and refuse to start the service with an error like:\n",
      "#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.\n",
      "\n",
      "# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other\n",
      "# container runtimes. If left unlimited, it may result in OOM issues with MySQL.\n",
      "ExecStart=\n",
      "ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 \n",
      "ExecReload=/bin/kill -s HUP $MAINPID\n",
      "\n",
      "# Having non-zero Limit*s causes performance problems due to accounting overhead\n",
      "# in the kernel. We recommend using cgroups to do container-local accounting.\n",
      "LimitNOFILE=infinity\n",
      "LimitNPROC=infinity\n",
      "LimitCORE=infinity\n",
      "\n",
      "# Uncomment TasksMax if your systemd version supports it.\n",
      "# Only systemd 226 and above support this version.\n",
      "TasksMax=infinity\n",
      "TimeoutStartSec=0\n",
      "\n",
      "# set delegate yes so that systemd does not reset the cgroups of docker containers\n",
      "Delegate=yes\n",
      "\n",
      "# kill only the docker process, not all processes in the cgroup\n",
      "KillMode=process\n",
      "\n",
      "[Install]\n",
      "WantedBy=multi-user.target\n",
      "\n",
      "I0318 14:53:11.572767    2593 cli_runner.go:164] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"22/tcp\") 0).HostPort}}'\" minikube\n",
      "I0318 14:53:11.600643    2593 main.go:141] libmachine: Using SSH client type: native\n",
      "I0318 14:53:11.600965    2593 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050b8790] 0x1050bafd0 <nil>  [] 0s} 127.0.0.1 52320 <nil> <nil>}\n",
      "I0318 14:53:11.600976    2593 main.go:141] libmachine: About to run SSH command:\n",
      "sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }\n",
      "I0318 14:53:11.960914    2593 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service\t2024-12-17 15:44:16.000000000 +0000\n",
      "+++ /lib/systemd/system/docker.service.new\t2025-03-18 06:53:11.569571013 +0000\n",
      "@@ -1,46 +1,49 @@\n",
      " [Unit]\n",
      " Description=Docker Application Container Engine\n",
      " Documentation=https://docs.docker.com\n",
      "-After=network-online.target docker.socket firewalld.service containerd.service time-set.target\n",
      "-Wants=network-online.target containerd.service\n",
      "+BindsTo=containerd.service\n",
      "+After=network-online.target firewalld.service containerd.service\n",
      "+Wants=network-online.target\n",
      " Requires=docker.socket\n",
      "+StartLimitBurst=3\n",
      "+StartLimitIntervalSec=60\n",
      " \n",
      " [Service]\n",
      " Type=notify\n",
      "-# the default is not to use systemd for cgroups because the delegate issues still\n",
      "-# exists and systemd currently does not support the cgroup feature set required\n",
      "-# for containers run by docker\n",
      "-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock\n",
      "-ExecReload=/bin/kill -s HUP $MAINPID\n",
      "-TimeoutStartSec=0\n",
      "-RestartSec=2\n",
      "-Restart=always\n",
      "+Restart=on-failure\n",
      " \n",
      "-# Note that StartLimit* options were moved from \"Service\" to \"Unit\" in systemd 229.\n",
      "-# Both the old, and new location are accepted by systemd 229 and up, so using the old location\n",
      "-# to make them work for either version of systemd.\n",
      "-StartLimitBurst=3\n",
      " \n",
      "-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.\n",
      "-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make\n",
      "-# this option work for either version of systemd.\n",
      "-StartLimitInterval=60s\n",
      "+\n",
      "+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.\n",
      "+# The base configuration already specifies an 'ExecStart=...' command. The first directive\n",
      "+# here is to clear out that command inherited from the base configuration. Without this,\n",
      "+# the command from the base configuration and the command specified here are treated as\n",
      "+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd\n",
      "+# will catch this invalid input and refuse to start the service with an error like:\n",
      "+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.\n",
      "+\n",
      "+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other\n",
      "+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.\n",
      "+ExecStart=\n",
      "+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 \n",
      "+ExecReload=/bin/kill -s HUP $MAINPID\n",
      " \n",
      " # Having non-zero Limit*s causes performance problems due to accounting overhead\n",
      " # in the kernel. We recommend using cgroups to do container-local accounting.\n",
      "+LimitNOFILE=infinity\n",
      " LimitNPROC=infinity\n",
      " LimitCORE=infinity\n",
      " \n",
      "-# Comment TasksMax if your systemd version does not support it.\n",
      "-# Only systemd 226 and above support this option.\n",
      "+# Uncomment TasksMax if your systemd version supports it.\n",
      "+# Only systemd 226 and above support this version.\n",
      " TasksMax=infinity\n",
      "+TimeoutStartSec=0\n",
      " \n",
      " # set delegate yes so that systemd does not reset the cgroups of docker containers\n",
      " Delegate=yes\n",
      " \n",
      " # kill only the docker process, not all processes in the cgroup\n",
      " KillMode=process\n",
      "-OOMScoreAdjust=-500\n",
      " \n",
      " [Install]\n",
      " WantedBy=multi-user.target\n",
      "Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.\n",
      "Executing: /lib/systemd/systemd-sysv-install enable docker\n",
      "\n",
      "I0318 14:53:11.960935    2593 machine.go:96] duration metric: took 1.333096833s to provisionDockerMachine\n",
      "I0318 14:53:11.960944    2593 client.go:171] duration metric: took 3.38413825s to LocalClient.Create\n",
      "I0318 14:53:11.960976    2593 start.go:167] duration metric: took 3.384178459s to libmachine.API.Create \"minikube\"\n",
      "I0318 14:53:11.960982    2593 start.go:293] postStartSetup for \"minikube\" (driver=\"docker\")\n",
      "I0318 14:53:11.961000    2593 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]\n",
      "I0318 14:53:11.961111    2593 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs\n",
      "I0318 14:53:11.961195    2593 cli_runner.go:164] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"22/tcp\") 0).HostPort}}'\" minikube\n",
      "I0318 14:53:11.990983    2593 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52320 SSHKeyPath:/Users/kz.li/.minikube/machines/minikube/id_rsa Username:docker}\n",
      "I0318 14:53:12.065506    2593 ssh_runner.go:195] Run: cat /etc/os-release\n",
      "I0318 14:53:12.067259    2593 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found\n",
      "I0318 14:53:12.067295    2593 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found\n",
      "I0318 14:53:12.067310    2593 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found\n",
      "I0318 14:53:12.067315    2593 info.go:137] Remote host: Ubuntu 22.04.5 LTS\n",
      "I0318 14:53:12.067327    2593 filesync.go:126] Scanning /Users/kz.li/.minikube/addons for local assets ...\n",
      "I0318 14:53:12.067614    2593 filesync.go:126] Scanning /Users/kz.li/.minikube/files for local assets ...\n",
      "I0318 14:53:12.067743    2593 start.go:296] duration metric: took 106.752792ms for postStartSetup\n",
      "I0318 14:53:12.069281    2593 cli_runner.go:164] Run: docker container inspect -f \"{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}\" minikube\n",
      "I0318 14:53:12.091177    2593 profile.go:143] Saving config to /Users/kz.li/.minikube/profiles/minikube/config.json ...\n",
      "I0318 14:53:12.091988    2593 ssh_runner.go:195] Run: sh -c \"df -h /var | awk 'NR==2{print $5}'\"\n",
      "I0318 14:53:12.092034    2593 cli_runner.go:164] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"22/tcp\") 0).HostPort}}'\" minikube\n",
      "I0318 14:53:12.112161    2593 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52320 SSHKeyPath:/Users/kz.li/.minikube/machines/minikube/id_rsa Username:docker}\n",
      "I0318 14:53:12.189500    2593 ssh_runner.go:195] Run: sh -c \"df -BG /var | awk 'NR==2{print $4}'\"\n",
      "I0318 14:53:12.192553    2593 start.go:128] duration metric: took 3.620541541s to createHost\n",
      "I0318 14:53:12.192573    2593 start.go:83] releasing machines lock for \"minikube\", held for 3.620613042s\n",
      "I0318 14:53:12.192766    2593 cli_runner.go:164] Run: docker container inspect -f \"{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}\" minikube\n",
      "I0318 14:53:12.217532    2593 ssh_runner.go:195] Run: cat /version.json\n",
      "I0318 14:53:12.217641    2593 cli_runner.go:164] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"22/tcp\") 0).HostPort}}'\" minikube\n",
      "I0318 14:53:12.218448    2593 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/\n",
      "I0318 14:53:12.218630    2593 cli_runner.go:164] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"22/tcp\") 0).HostPort}}'\" minikube\n",
      "I0318 14:53:12.239839    2593 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52320 SSHKeyPath:/Users/kz.li/.minikube/machines/minikube/id_rsa Username:docker}\n",
      "I0318 14:53:12.241130    2593 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52320 SSHKeyPath:/Users/kz.li/.minikube/machines/minikube/id_rsa Username:docker}\n",
      "I0318 14:53:12.317984    2593 ssh_runner.go:195] Run: systemctl --version\n",
      "I0318 14:53:13.632527    2593 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.414004625s)\n",
      "I0318 14:53:13.632595    2593 ssh_runner.go:235] Completed: systemctl --version: (1.31458825s)\n",
      "I0318 14:53:13.633081    2593 ssh_runner.go:195] Run: sh -c \"stat /etc/cni/net.d/*loopback.conf*\"\n",
      "I0318 14:53:13.640425    2593 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c \"grep -q loopback {} && ( grep -q name {} || sudo sed -i '/\"type\": \"loopback\"/i \\ \\ \\ \\ \"name\": \"loopback\",' {} ) && sudo sed -i 's|\"cniVersion\": \".*\"|\"cniVersion\": \"1.0.0\"|g' {}\" ;\n",
      "I0318 14:53:13.661233    2593 cni.go:230] loopback cni configuration patched: \"/etc/cni/net.d/*loopback.conf*\" found\n",
      "I0318 14:53:13.661473    2593 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf \"%p, \" -exec sh -c \"sudo mv {} {}.mk_disabled\" ;\n",
      "I0318 14:53:13.673581    2593 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)\n",
      "I0318 14:53:13.673594    2593 start.go:495] detecting cgroup driver to use...\n",
      "I0318 14:53:13.673611    2593 detect.go:187] detected \"cgroupfs\" cgroup driver on host os\n",
      "I0318 14:53:13.673778    2593 ssh_runner.go:195] Run: /bin/bash -c \"sudo mkdir -p /etc && printf %s \"runtime-endpoint: unix:///run/containerd/containerd.sock\n",
      "\" | sudo tee /etc/crictl.yaml\"\n",
      "I0318 14:53:13.680462    2593 ssh_runner.go:195] Run: sh -c \"sudo sed -i -r 's|^( *)sandbox_image = .*$|\\1sandbox_image = \"registry.k8s.io/pause:3.10\"|' /etc/containerd/config.toml\"\n",
      "I0318 14:53:13.685243    2593 ssh_runner.go:195] Run: sh -c \"sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\\1restrict_oom_score_adj = false|' /etc/containerd/config.toml\"\n",
      "I0318 14:53:13.689178    2593 containerd.go:146] configuring containerd to use \"cgroupfs\" as cgroup driver...\n",
      "I0318 14:53:13.689349    2593 ssh_runner.go:195] Run: sh -c \"sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\\1SystemdCgroup = false|g' /etc/containerd/config.toml\"\n",
      "I0318 14:53:13.693569    2593 ssh_runner.go:195] Run: sh -c \"sudo sed -i 's|\"io.containerd.runtime.v1.linux\"|\"io.containerd.runc.v2\"|g' /etc/containerd/config.toml\"\n",
      "I0318 14:53:13.697440    2593 ssh_runner.go:195] Run: sh -c \"sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml\"\n",
      "I0318 14:53:13.701267    2593 ssh_runner.go:195] Run: sh -c \"sudo sed -i 's|\"io.containerd.runc.v1\"|\"io.containerd.runc.v2\"|g' /etc/containerd/config.toml\"\n",
      "I0318 14:53:13.704992    2593 ssh_runner.go:195] Run: sh -c \"sudo rm -rf /etc/cni/net.mk\"\n",
      "I0318 14:53:13.708654    2593 ssh_runner.go:195] Run: sh -c \"sudo sed -i -r 's|^( *)conf_dir = .*$|\\1conf_dir = \"/etc/cni/net.d\"|g' /etc/containerd/config.toml\"\n",
      "I0318 14:53:13.712343    2593 ssh_runner.go:195] Run: sh -c \"sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml\"\n",
      "I0318 14:53:13.715743    2593 ssh_runner.go:195] Run: sh -c \"sudo sed -i -r 's|^( *)\\[plugins.\"io.containerd.grpc.v1.cri\"\\]|&\\n\\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml\"\n",
      "I0318 14:53:13.719262    2593 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables\n",
      "I0318 14:53:13.722557    2593 ssh_runner.go:195] Run: sudo sh -c \"echo 1 > /proc/sys/net/ipv4/ip_forward\"\n",
      "I0318 14:53:13.725911    2593 ssh_runner.go:195] Run: sudo systemctl daemon-reload\n",
      "I0318 14:53:13.750312    2593 ssh_runner.go:195] Run: sudo systemctl restart containerd\n",
      "I0318 14:53:13.799819    2593 start.go:495] detecting cgroup driver to use...\n",
      "I0318 14:53:13.799857    2593 detect.go:187] detected \"cgroupfs\" cgroup driver on host os\n",
      "I0318 14:53:13.799975    2593 ssh_runner.go:195] Run: sudo systemctl cat docker.service\n",
      "I0318 14:53:13.805038    2593 cruntime.go:279] skipping containerd shutdown because we are bound to it\n",
      "I0318 14:53:13.805116    2593 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio\n",
      "I0318 14:53:13.809974    2593 ssh_runner.go:195] Run: /bin/bash -c \"sudo mkdir -p /etc && printf %s \"runtime-endpoint: unix:///var/run/cri-dockerd.sock\n",
      "\" | sudo tee /etc/crictl.yaml\"\n",
      "I0318 14:53:13.816035    2593 ssh_runner.go:195] Run: which cri-dockerd\n",
      "I0318 14:53:13.817724    2593 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d\n",
      "I0318 14:53:13.824838    2593 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)\n",
      "I0318 14:53:13.831558    2593 ssh_runner.go:195] Run: sudo systemctl unmask docker.service\n",
      "I0318 14:53:13.859850    2593 ssh_runner.go:195] Run: sudo systemctl enable docker.socket\n",
      "I0318 14:53:13.888108    2593 docker.go:574] configuring docker to use \"cgroupfs\" as cgroup driver...\n",
      "I0318 14:53:13.888224    2593 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)\n",
      "I0318 14:53:13.895165    2593 ssh_runner.go:195] Run: sudo systemctl daemon-reload\n",
      "I0318 14:53:13.936811    2593 ssh_runner.go:195] Run: sudo systemctl restart docker\n",
      "I0318 14:53:14.048723    2593 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket\n",
      "I0318 14:53:14.053407    2593 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service\n",
      "I0318 14:53:14.058159    2593 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket\n",
      "I0318 14:53:14.085662    2593 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket\n",
      "I0318 14:53:14.112151    2593 ssh_runner.go:195] Run: sudo systemctl daemon-reload\n",
      "I0318 14:53:14.139123    2593 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket\n",
      "I0318 14:53:14.159829    2593 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service\n",
      "I0318 14:53:14.164352    2593 ssh_runner.go:195] Run: sudo systemctl daemon-reload\n",
      "I0318 14:53:14.190433    2593 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service\n",
      "I0318 14:53:14.225758    2593 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock\n",
      "I0318 14:53:14.226126    2593 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock\n",
      "I0318 14:53:14.227920    2593 start.go:563] Will wait 60s for crictl version\n",
      "I0318 14:53:14.228002    2593 ssh_runner.go:195] Run: which crictl\n",
      "I0318 14:53:14.229551    2593 ssh_runner.go:195] Run: sudo /usr/bin/crictl version\n",
      "I0318 14:53:14.242342    2593 start.go:579] Version:  0.1.0\n",
      "RuntimeName:  docker\n",
      "RuntimeVersion:  27.4.1\n",
      "RuntimeApiVersion:  v1\n",
      "I0318 14:53:14.242557    2593 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}\n",
      "I0318 14:53:14.251845    2593 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}\n",
      "I0318 14:53:14.273038    2593 out.go:235] * Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...\n",
      "I0318 14:53:14.273302    2593 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal\n",
      "I0318 14:53:14.350090    2593 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254\n",
      "I0318 14:53:14.350668    2593 ssh_runner.go:195] Run: grep 192.168.65.254\thost.minikube.internal$ /etc/hosts\n",
      "I0318 14:53:14.352760    2593 ssh_runner.go:195] Run: /bin/bash -c \"{ grep -v $'\\thost.minikube.internal$' \"/etc/hosts\"; echo \"192.168.65.254\thost.minikube.internal\"; } > /tmp/h.$$; sudo cp /tmp/h.$$ \"/etc/hosts\"\"\n",
      "I0318 14:53:14.357301    2593 cli_runner.go:164] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"8443/tcp\") 0).HostPort}}'\" minikube\n",
      "I0318 14:53:14.379736    2593 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:7788 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...\n",
      "I0318 14:53:14.379821    2593 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker\n",
      "I0318 14:53:14.379873    2593 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}\n",
      "I0318 14:53:14.389639    2593 docker.go:689] Got preloaded images: -- stdout --\n",
      "registry.k8s.io/kube-apiserver:v1.32.0\n",
      "registry.k8s.io/kube-controller-manager:v1.32.0\n",
      "registry.k8s.io/kube-scheduler:v1.32.0\n",
      "registry.k8s.io/kube-proxy:v1.32.0\n",
      "registry.k8s.io/etcd:3.5.16-0\n",
      "registry.k8s.io/coredns/coredns:v1.11.3\n",
      "registry.k8s.io/pause:3.10\n",
      "gcr.io/k8s-minikube/storage-provisioner:v5\n",
      "\n",
      "-- /stdout --\n",
      "I0318 14:53:14.389652    2593 docker.go:619] Images already preloaded, skipping extraction\n",
      "I0318 14:53:14.389757    2593 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}\n",
      "I0318 14:53:14.397821    2593 docker.go:689] Got preloaded images: -- stdout --\n",
      "registry.k8s.io/kube-apiserver:v1.32.0\n",
      "registry.k8s.io/kube-scheduler:v1.32.0\n",
      "registry.k8s.io/kube-controller-manager:v1.32.0\n",
      "registry.k8s.io/kube-proxy:v1.32.0\n",
      "registry.k8s.io/etcd:3.5.16-0\n",
      "registry.k8s.io/coredns/coredns:v1.11.3\n",
      "registry.k8s.io/pause:3.10\n",
      "gcr.io/k8s-minikube/storage-provisioner:v5\n",
      "\n",
      "-- /stdout --\n",
      "I0318 14:53:14.397829    2593 cache_images.go:84] Images are preloaded, skipping loading\n",
      "I0318 14:53:14.397836    2593 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...\n",
      "I0318 14:53:14.397911    2593 kubeadm.go:946] kubelet [Unit]\n",
      "Wants=docker.socket\n",
      "\n",
      "[Service]\n",
      "ExecStart=\n",
      "ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2\n",
      "\n",
      "[Install]\n",
      " config:\n",
      "{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}\n",
      "I0318 14:53:14.397988    2593 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}\n",
      "I0318 14:53:14.418422    2593 cni.go:84] Creating CNI manager for \"\"\n",
      "I0318 14:53:14.418435    2593 cni.go:158] \"docker\" driver + \"docker\" container runtime found on kubernetes v1.24+, recommending bridge\n",
      "I0318 14:53:14.418452    2593 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16\n",
      "I0318 14:53:14.418472    2593 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:[\"127.0.0.1\", \"localhost\", \"192.168.49.2\"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}\n",
      "I0318 14:53:14.418603    2593 kubeadm.go:195] kubeadm config:\n",
      "apiVersion: kubeadm.k8s.io/v1beta4\n",
      "kind: InitConfiguration\n",
      "localAPIEndpoint:\n",
      "  advertiseAddress: 192.168.49.2\n",
      "  bindPort: 8443\n",
      "bootstrapTokens:\n",
      "  - groups:\n",
      "      - system:bootstrappers:kubeadm:default-node-token\n",
      "    ttl: 24h0m0s\n",
      "    usages:\n",
      "      - signing\n",
      "      - authentication\n",
      "nodeRegistration:\n",
      "  criSocket: unix:///var/run/cri-dockerd.sock\n",
      "  name: \"minikube\"\n",
      "  kubeletExtraArgs:\n",
      "    - name: \"node-ip\"\n",
      "      value: \"192.168.49.2\"\n",
      "  taints: []\n",
      "---\n",
      "apiVersion: kubeadm.k8s.io/v1beta4\n",
      "kind: ClusterConfiguration\n",
      "apiServer:\n",
      "  certSANs: [\"127.0.0.1\", \"localhost\", \"192.168.49.2\"]\n",
      "  extraArgs:\n",
      "    - name: \"enable-admission-plugins\"\n",
      "      value: \"NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota\"\n",
      "controllerManager:\n",
      "  extraArgs:\n",
      "    - name: \"allocate-node-cidrs\"\n",
      "      value: \"true\"\n",
      "    - name: \"leader-elect\"\n",
      "      value: \"false\"\n",
      "scheduler:\n",
      "  extraArgs:\n",
      "    - name: \"leader-elect\"\n",
      "      value: \"false\"\n",
      "certificatesDir: /var/lib/minikube/certs\n",
      "clusterName: mk\n",
      "controlPlaneEndpoint: control-plane.minikube.internal:8443\n",
      "etcd:\n",
      "  local:\n",
      "    dataDir: /var/lib/minikube/etcd\n",
      "    extraArgs:\n",
      "      - name: \"proxy-refresh-interval\"\n",
      "        value: \"70000\"\n",
      "kubernetesVersion: v1.32.0\n",
      "networking:\n",
      "  dnsDomain: cluster.local\n",
      "  podSubnet: \"10.244.0.0/16\"\n",
      "  serviceSubnet: 10.96.0.0/12\n",
      "---\n",
      "apiVersion: kubelet.config.k8s.io/v1beta1\n",
      "kind: KubeletConfiguration\n",
      "authentication:\n",
      "  x509:\n",
      "    clientCAFile: /var/lib/minikube/certs/ca.crt\n",
      "cgroupDriver: cgroupfs\n",
      "containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock\n",
      "hairpinMode: hairpin-veth\n",
      "runtimeRequestTimeout: 15m\n",
      "clusterDomain: \"cluster.local\"\n",
      "# disable disk resource management by default\n",
      "imageGCHighThresholdPercent: 100\n",
      "evictionHard:\n",
      "  nodefs.available: \"0%\"\n",
      "  nodefs.inodesFree: \"0%\"\n",
      "  imagefs.available: \"0%\"\n",
      "failSwapOn: false\n",
      "staticPodPath: /etc/kubernetes/manifests\n",
      "---\n",
      "apiVersion: kubeproxy.config.k8s.io/v1alpha1\n",
      "kind: KubeProxyConfiguration\n",
      "clusterCIDR: \"10.244.0.0/16\"\n",
      "metricsBindAddress: 0.0.0.0:10249\n",
      "conntrack:\n",
      "  maxPerCore: 0\n",
      "# Skip setting \"net.netfilter.nf_conntrack_tcp_timeout_established\"\n",
      "  tcpEstablishedTimeout: 0s\n",
      "# Skip setting \"net.netfilter.nf_conntrack_tcp_timeout_close\"\n",
      "  tcpCloseWaitTimeout: 0s\n",
      "\n",
      "I0318 14:53:14.418741    2593 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0\n",
      "I0318 14:53:14.422174    2593 binaries.go:44] Found k8s binaries, skipping transfer\n",
      "I0318 14:53:14.422268    2593 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube\n",
      "I0318 14:53:14.425354    2593 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)\n",
      "I0318 14:53:14.431334    2593 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)\n",
      "I0318 14:53:14.437026    2593 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)\n",
      "I0318 14:53:14.442797    2593 ssh_runner.go:195] Run: grep 192.168.49.2\tcontrol-plane.minikube.internal$ /etc/hosts\n",
      "I0318 14:53:14.444259    2593 ssh_runner.go:195] Run: /bin/bash -c \"{ grep -v $'\\tcontrol-plane.minikube.internal$' \"/etc/hosts\"; echo \"192.168.49.2\tcontrol-plane.minikube.internal\"; } > /tmp/h.$$; sudo cp /tmp/h.$$ \"/etc/hosts\"\"\n",
      "I0318 14:53:14.447962    2593 ssh_runner.go:195] Run: sudo systemctl daemon-reload\n",
      "I0318 14:53:14.472722    2593 ssh_runner.go:195] Run: sudo systemctl start kubelet\n",
      "I0318 14:53:14.491729    2593 certs.go:68] Setting up /Users/kz.li/.minikube/profiles/minikube for IP: 192.168.49.2\n",
      "I0318 14:53:14.491754    2593 certs.go:194] generating shared ca certs ...\n",
      "I0318 14:53:14.491772    2593 certs.go:226] acquiring lock for ca certs: {Name:mkfab4d31cb6b9d9d8827cafb4c88dbb4b0411a4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}\n",
      "I0318 14:53:14.492619    2593 certs.go:235] skipping valid \"minikubeCA\" ca cert: /Users/kz.li/.minikube/ca.key\n",
      "I0318 14:53:14.492963    2593 certs.go:235] skipping valid \"proxyClientCA\" ca cert: /Users/kz.li/.minikube/proxy-client-ca.key\n",
      "I0318 14:53:14.492984    2593 certs.go:256] generating profile certs ...\n",
      "I0318 14:53:14.493073    2593 certs.go:363] generating signed profile cert for \"minikube-user\": /Users/kz.li/.minikube/profiles/minikube/client.key\n",
      "I0318 14:53:14.493112    2593 crypto.go:68] Generating cert /Users/kz.li/.minikube/profiles/minikube/client.crt with IP's: []\n",
      "I0318 14:53:14.585522    2593 crypto.go:156] Writing cert to /Users/kz.li/.minikube/profiles/minikube/client.crt ...\n",
      "I0318 14:53:14.585529    2593 lock.go:35] WriteFile acquiring /Users/kz.li/.minikube/profiles/minikube/client.crt: {Name:mk8668b18827f6b1c587f65cf55cc0d05de4c541 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}\n",
      "I0318 14:53:14.585893    2593 crypto.go:164] Writing key to /Users/kz.li/.minikube/profiles/minikube/client.key ...\n",
      "I0318 14:53:14.585895    2593 lock.go:35] WriteFile acquiring /Users/kz.li/.minikube/profiles/minikube/client.key: {Name:mk8784156e54f64fa39363c39fbc8c272f1eb722 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}\n",
      "I0318 14:53:14.586049    2593 certs.go:363] generating signed profile cert for \"minikube\": /Users/kz.li/.minikube/profiles/minikube/apiserver.key.7fb57e3c\n",
      "I0318 14:53:14.586055    2593 crypto.go:68] Generating cert /Users/kz.li/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]\n",
      "I0318 14:53:14.634641    2593 crypto.go:156] Writing cert to /Users/kz.li/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...\n",
      "I0318 14:53:14.634644    2593 lock.go:35] WriteFile acquiring /Users/kz.li/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk93d082a8e6f11d0fca0968e876f6717a9f1784 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}\n",
      "I0318 14:53:14.634885    2593 crypto.go:164] Writing key to /Users/kz.li/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...\n",
      "I0318 14:53:14.634888    2593 lock.go:35] WriteFile acquiring /Users/kz.li/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mk5542e9588aed1f1aebe49b1b9877cb54d80b51 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}\n",
      "I0318 14:53:14.635021    2593 certs.go:381] copying /Users/kz.li/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /Users/kz.li/.minikube/profiles/minikube/apiserver.crt\n",
      "I0318 14:53:14.635144    2593 certs.go:385] copying /Users/kz.li/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /Users/kz.li/.minikube/profiles/minikube/apiserver.key\n",
      "I0318 14:53:14.635255    2593 certs.go:363] generating signed profile cert for \"aggregator\": /Users/kz.li/.minikube/profiles/minikube/proxy-client.key\n",
      "I0318 14:53:14.635263    2593 crypto.go:68] Generating cert /Users/kz.li/.minikube/profiles/minikube/proxy-client.crt with IP's: []\n",
      "I0318 14:53:14.735731    2593 crypto.go:156] Writing cert to /Users/kz.li/.minikube/profiles/minikube/proxy-client.crt ...\n",
      "I0318 14:53:14.735735    2593 lock.go:35] WriteFile acquiring /Users/kz.li/.minikube/profiles/minikube/proxy-client.crt: {Name:mk3f886682abdbbbfe03699207230f9fea42da7c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}\n",
      "I0318 14:53:14.736023    2593 crypto.go:164] Writing key to /Users/kz.li/.minikube/profiles/minikube/proxy-client.key ...\n",
      "I0318 14:53:14.736025    2593 lock.go:35] WriteFile acquiring /Users/kz.li/.minikube/profiles/minikube/proxy-client.key: {Name:mkb066210428be873d8c94e7139f53eab164972b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}\n",
      "I0318 14:53:14.736306    2593 certs.go:484] found cert: /Users/kz.li/.minikube/certs/ca-key.pem (1675 bytes)\n",
      "I0318 14:53:14.736328    2593 certs.go:484] found cert: /Users/kz.li/.minikube/certs/ca.pem (1074 bytes)\n",
      "I0318 14:53:14.736347    2593 certs.go:484] found cert: /Users/kz.li/.minikube/certs/cert.pem (1119 bytes)\n",
      "I0318 14:53:14.736364    2593 certs.go:484] found cert: /Users/kz.li/.minikube/certs/key.pem (1675 bytes)\n",
      "I0318 14:53:14.736665    2593 ssh_runner.go:362] scp /Users/kz.li/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)\n",
      "I0318 14:53:14.761889    2593 ssh_runner.go:362] scp /Users/kz.li/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)\n",
      "I0318 14:53:14.774097    2593 ssh_runner.go:362] scp /Users/kz.li/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)\n",
      "I0318 14:53:14.784783    2593 ssh_runner.go:362] scp /Users/kz.li/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)\n",
      "I0318 14:53:14.794630    2593 ssh_runner.go:362] scp /Users/kz.li/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)\n",
      "I0318 14:53:14.803933    2593 ssh_runner.go:362] scp /Users/kz.li/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)\n",
      "I0318 14:53:14.812624    2593 ssh_runner.go:362] scp /Users/kz.li/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)\n",
      "I0318 14:53:14.820745    2593 ssh_runner.go:362] scp /Users/kz.li/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)\n",
      "I0318 14:53:14.828461    2593 ssh_runner.go:362] scp /Users/kz.li/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)\n",
      "I0318 14:53:14.838321    2593 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)\n",
      "I0318 14:53:14.844145    2593 ssh_runner.go:195] Run: openssl version\n",
      "I0318 14:53:14.846586    2593 ssh_runner.go:195] Run: sudo /bin/bash -c \"test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem\"\n",
      "I0318 14:53:14.852274    2593 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem\n",
      "I0318 14:53:14.853576    2593 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Feb 19  2024 /usr/share/ca-certificates/minikubeCA.pem\n",
      "I0318 14:53:14.853594    2593 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem\n",
      "I0318 14:53:14.856091    2593 ssh_runner.go:195] Run: sudo /bin/bash -c \"test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0\"\n",
      "I0318 14:53:14.859100    2593 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt\n",
      "I0318 14:53:14.860651    2593 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1\n",
      "stdout:\n",
      "\n",
      "stderr:\n",
      "stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory\n",
      "I0318 14:53:14.860738    2593 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:7788 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}\n",
      "I0318 14:53:14.860803    2593 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}\n",
      "I0318 14:53:14.868774    2593 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd\n",
      "I0318 14:53:14.872041    2593 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml\n",
      "I0318 14:53:14.874898    2593 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver\n",
      "I0318 14:53:14.874934    2593 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf\n",
      "I0318 14:53:14.877862    2593 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2\n",
      "stdout:\n",
      "\n",
      "stderr:\n",
      "ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory\n",
      "ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory\n",
      "ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory\n",
      "ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory\n",
      "I0318 14:53:14.877868    2593 kubeadm.go:157] found existing configuration files:\n",
      "\n",
      "I0318 14:53:14.877901    2593 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf\n",
      "I0318 14:53:14.881399    2593 kubeadm.go:163] \"https://control-plane.minikube.internal:8443\" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2\n",
      "stdout:\n",
      "\n",
      "stderr:\n",
      "grep: /etc/kubernetes/admin.conf: No such file or directory\n",
      "I0318 14:53:14.881432    2593 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf\n",
      "I0318 14:53:14.884232    2593 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf\n",
      "I0318 14:53:14.887138    2593 kubeadm.go:163] \"https://control-plane.minikube.internal:8443\" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2\n",
      "stdout:\n",
      "\n",
      "stderr:\n",
      "grep: /etc/kubernetes/kubelet.conf: No such file or directory\n",
      "I0318 14:53:14.887164    2593 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf\n",
      "I0318 14:53:14.890096    2593 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf\n",
      "I0318 14:53:14.892955    2593 kubeadm.go:163] \"https://control-plane.minikube.internal:8443\" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2\n",
      "stdout:\n",
      "\n",
      "stderr:\n",
      "grep: /etc/kubernetes/controller-manager.conf: No such file or directory\n",
      "I0318 14:53:14.892981    2593 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf\n",
      "I0318 14:53:14.895680    2593 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf\n",
      "I0318 14:53:14.898598    2593 kubeadm.go:163] \"https://control-plane.minikube.internal:8443\" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2\n",
      "stdout:\n",
      "\n",
      "stderr:\n",
      "grep: /etc/kubernetes/scheduler.conf: No such file or directory\n",
      "I0318 14:53:14.898630    2593 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf\n",
      "I0318 14:53:14.901435    2593 ssh_runner.go:286] Start: /bin/bash -c \"sudo env PATH=\"/var/lib/minikube/binaries/v1.32.0:$PATH\" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables\"\n",
      "I0318 14:53:14.921134    2593 kubeadm.go:310] [init] Using Kubernetes version: v1.32.0\n",
      "I0318 14:53:14.921160    2593 kubeadm.go:310] [preflight] Running pre-flight checks\n",
      "I0318 14:53:14.954075    2593 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster\n",
      "I0318 14:53:14.954183    2593 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection\n",
      "I0318 14:53:14.954276    2593 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'\n",
      "I0318 14:53:14.958673    2593 kubeadm.go:310] [certs] Using certificateDir folder \"/var/lib/minikube/certs\"\n",
      "I0318 14:53:14.962283    2593 out.go:235]   - Generating certificates and keys ...\n",
      "I0318 14:53:14.962389    2593 kubeadm.go:310] [certs] Using existing ca certificate authority\n",
      "I0318 14:53:14.962463    2593 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk\n",
      "I0318 14:53:15.080333    2593 kubeadm.go:310] [certs] Generating \"apiserver-kubelet-client\" certificate and key\n",
      "I0318 14:53:15.208279    2593 kubeadm.go:310] [certs] Generating \"front-proxy-ca\" certificate and key\n",
      "I0318 14:53:15.324745    2593 kubeadm.go:310] [certs] Generating \"front-proxy-client\" certificate and key\n",
      "I0318 14:53:15.384012    2593 kubeadm.go:310] [certs] Generating \"etcd/ca\" certificate and key\n",
      "I0318 14:53:15.463581    2593 kubeadm.go:310] [certs] Generating \"etcd/server\" certificate and key\n",
      "I0318 14:53:15.464053    2593 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]\n",
      "I0318 14:53:15.484822    2593 kubeadm.go:310] [certs] Generating \"etcd/peer\" certificate and key\n",
      "I0318 14:53:15.485046    2593 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]\n",
      "I0318 14:53:15.694062    2593 kubeadm.go:310] [certs] Generating \"etcd/healthcheck-client\" certificate and key\n",
      "I0318 14:53:15.737911    2593 kubeadm.go:310] [certs] Generating \"apiserver-etcd-client\" certificate and key\n",
      "I0318 14:53:15.778020    2593 kubeadm.go:310] [certs] Generating \"sa\" key and public key\n",
      "I0318 14:53:15.778153    2593 kubeadm.go:310] [kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"\n",
      "I0318 14:53:15.805188    2593 kubeadm.go:310] [kubeconfig] Writing \"admin.conf\" kubeconfig file\n",
      "I0318 14:53:15.885661    2593 kubeadm.go:310] [kubeconfig] Writing \"super-admin.conf\" kubeconfig file\n",
      "I0318 14:53:16.011432    2593 kubeadm.go:310] [kubeconfig] Writing \"kubelet.conf\" kubeconfig file\n",
      "I0318 14:53:16.100379    2593 kubeadm.go:310] [kubeconfig] Writing \"controller-manager.conf\" kubeconfig file\n",
      "I0318 14:53:16.180720    2593 kubeadm.go:310] [kubeconfig] Writing \"scheduler.conf\" kubeconfig file\n",
      "I0318 14:53:16.181028    2593 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"\n",
      "I0318 14:53:16.183053    2593 kubeadm.go:310] [control-plane] Using manifest folder \"/etc/kubernetes/manifests\"\n",
      "I0318 14:53:16.188761    2593 out.go:235]   - Booting up control plane ...\n",
      "I0318 14:53:16.188941    2593 kubeadm.go:310] [control-plane] Creating static Pod manifest for \"kube-apiserver\"\n",
      "I0318 14:53:16.189066    2593 kubeadm.go:310] [control-plane] Creating static Pod manifest for \"kube-controller-manager\"\n",
      "I0318 14:53:16.189174    2593 kubeadm.go:310] [control-plane] Creating static Pod manifest for \"kube-scheduler\"\n",
      "I0318 14:53:16.189329    2593 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n",
      "I0318 14:53:16.190786    2593 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n",
      "I0318 14:53:16.190822    2593 kubeadm.go:310] [kubelet-start] Starting the kubelet\n",
      "I0318 14:53:16.229888    2593 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\"\n",
      "I0318 14:53:16.229990    2593 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s\n",
      "I0318 14:53:16.732840    2593 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 502.644209ms\n",
      "I0318 14:53:16.732955    2593 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s\n",
      "I0318 14:53:19.736701    2593 kubeadm.go:310] [api-check] The API server is healthy after 3.004150751s\n",
      "I0318 14:53:19.741880    2593 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n",
      "I0318 14:53:19.745084    2593 kubeadm.go:310] [kubelet] Creating a ConfigMap \"kubelet-config\" in namespace kube-system with the configuration for the kubelets in the cluster\n",
      "I0318 14:53:19.750309    2593 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs\n",
      "I0318 14:53:19.750616    2593 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]\n",
      "I0318 14:53:19.752323    2593 kubeadm.go:310] [bootstrap-token] Using token: z1qm1d.anrhfwzfgyhe7gv5\n",
      "I0318 14:53:19.758341    2593 out.go:235]   - Configuring RBAC rules ...\n",
      "I0318 14:53:19.758570    2593 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\n",
      "I0318 14:53:19.763591    2593 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes\n",
      "I0318 14:53:19.765185    2593 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n",
      "I0318 14:53:19.765641    2593 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n",
      "I0318 14:53:19.766266    2593 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n",
      "I0318 14:53:19.766830    2593 kubeadm.go:310] [bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n",
      "I0318 14:53:20.144713    2593 kubeadm.go:310] [kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key\n",
      "I0318 14:53:20.561175    2593 kubeadm.go:310] [addons] Applied essential addon: CoreDNS\n",
      "I0318 14:53:21.144984    2593 kubeadm.go:310] [addons] Applied essential addon: kube-proxy\n",
      "I0318 14:53:21.146194    2593 kubeadm.go:310] \n",
      "I0318 14:53:21.146256    2593 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!\n",
      "I0318 14:53:21.146268    2593 kubeadm.go:310] \n",
      "I0318 14:53:21.146346    2593 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:\n",
      "I0318 14:53:21.146349    2593 kubeadm.go:310] \n",
      "I0318 14:53:21.146367    2593 kubeadm.go:310]   mkdir -p $HOME/.kube\n",
      "I0318 14:53:21.146433    2593 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n",
      "I0318 14:53:21.146469    2593 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config\n",
      "I0318 14:53:21.146473    2593 kubeadm.go:310] \n",
      "I0318 14:53:21.146533    2593 kubeadm.go:310] Alternatively, if you are the root user, you can run:\n",
      "I0318 14:53:21.146542    2593 kubeadm.go:310] \n",
      "I0318 14:53:21.146597    2593 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf\n",
      "I0318 14:53:21.146600    2593 kubeadm.go:310] \n",
      "I0318 14:53:21.146655    2593 kubeadm.go:310] You should now deploy a pod network to the cluster.\n",
      "I0318 14:53:21.146758    2593 kubeadm.go:310] Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n",
      "I0318 14:53:21.146816    2593 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/\n",
      "I0318 14:53:21.146821    2593 kubeadm.go:310] \n",
      "I0318 14:53:21.146920    2593 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities\n",
      "I0318 14:53:21.147003    2593 kubeadm.go:310] and service account keys on each node and then running the following as root:\n",
      "I0318 14:53:21.147029    2593 kubeadm.go:310] \n",
      "I0318 14:53:21.147121    2593 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token z1qm1d.anrhfwzfgyhe7gv5 \\\n",
      "I0318 14:53:21.147231    2593 kubeadm.go:310] \t--discovery-token-ca-cert-hash sha256:90de18ec28d5d09a33d88e0be444752b8db1b5882e0dd2127a45c00b3b784a00 \\\n",
      "I0318 14:53:21.147260    2593 kubeadm.go:310] \t--control-plane \n",
      "I0318 14:53:21.147269    2593 kubeadm.go:310] \n",
      "I0318 14:53:21.147431    2593 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:\n",
      "I0318 14:53:21.147434    2593 kubeadm.go:310] \n",
      "I0318 14:53:21.147506    2593 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token z1qm1d.anrhfwzfgyhe7gv5 \\\n",
      "I0318 14:53:21.147595    2593 kubeadm.go:310] \t--discovery-token-ca-cert-hash sha256:90de18ec28d5d09a33d88e0be444752b8db1b5882e0dd2127a45c00b3b784a00 \n",
      "I0318 14:53:21.149170    2593 kubeadm.go:310] \t[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node\n",
      "I0318 14:53:21.149278    2593 kubeadm.go:310] \t[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'\n",
      "I0318 14:53:21.149297    2593 cni.go:84] Creating CNI manager for \"\"\n",
      "I0318 14:53:21.149311    2593 cni.go:158] \"docker\" driver + \"docker\" container runtime found on kubernetes v1.24+, recommending bridge\n",
      "I0318 14:53:21.152683    2593 out.go:177] * Configuring bridge CNI (Container Networking Interface) ...\n",
      "I0318 14:53:21.157920    2593 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d\n",
      "I0318 14:53:21.165238    2593 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)\n",
      "I0318 14:53:21.175909    2593 ssh_runner.go:195] Run: /bin/bash -c \"cat /proc/$(pgrep kube-apiserver)/oom_adj\"\n",
      "I0318 14:53:21.176076    2593 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig\n",
      "I0318 14:53:21.176078    2593 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_03_18T14_53_21_0700 minikube.k8s.io/version=v1.35.0 minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed minikube.k8s.io/name=minikube minikube.k8s.io/primary=true\n",
      "I0318 14:53:21.180094    2593 ops.go:34] apiserver oom_adj: -16\n",
      "I0318 14:53:21.221947    2593 kubeadm.go:1113] duration metric: took 46.02275ms to wait for elevateKubeSystemPrivileges\n",
      "I0318 14:53:21.221979    2593 kubeadm.go:394] duration metric: took 6.361281375s to StartCluster\n",
      "I0318 14:53:21.222005    2593 settings.go:142] acquiring lock: {Name:mka9ec16c7a415a3a5b968cae8f32c2698017cfe Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}\n",
      "I0318 14:53:21.222361    2593 settings.go:150] Updating kubeconfig:  /Users/kz.li/.kube/config\n",
      "I0318 14:53:21.226704    2593 lock.go:35] WriteFile acquiring /Users/kz.li/.kube/config: {Name:mkdae5a299d6c770801888f9b416156bd68f0445 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}\n",
      "I0318 14:53:21.228046    2593 ssh_runner.go:195] Run: /bin/bash -c \"sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml\"\n",
      "I0318 14:53:21.228088    2593 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}\n",
      "I0318 14:53:21.228099    2593 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]\n",
      "I0318 14:53:21.228201    2593 addons.go:69] Setting storage-provisioner=true in profile \"minikube\"\n",
      "I0318 14:53:21.228217    2593 addons.go:238] Setting addon storage-provisioner=true in \"minikube\"\n",
      "I0318 14:53:21.228231    2593 addons.go:69] Setting default-storageclass=true in profile \"minikube\"\n",
      "I0318 14:53:21.228243    2593 host.go:66] Checking if \"minikube\" exists ...\n",
      "I0318 14:53:21.228258    2593 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on \"minikube\"\n",
      "I0318 14:53:21.228373    2593 config.go:182] Loaded profile config \"minikube\": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0\n",
      "I0318 14:53:21.228797    2593 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}\n",
      "I0318 14:53:21.228933    2593 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}\n",
      "I0318 14:53:21.232121    2593 out.go:177] * Verifying Kubernetes components...\n",
      "I0318 14:53:21.237781    2593 ssh_runner.go:195] Run: sudo systemctl daemon-reload\n",
      "I0318 14:53:21.251067    2593 addons.go:238] Setting addon default-storageclass=true in \"minikube\"\n",
      "I0318 14:53:21.251084    2593 host.go:66] Checking if \"minikube\" exists ...\n",
      "I0318 14:53:21.251411    2593 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}\n",
      "I0318 14:53:21.255651    2593 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5\n",
      "I0318 14:53:21.258771    2593 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml\n",
      "I0318 14:53:21.258775    2593 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)\n",
      "I0318 14:53:21.258832    2593 cli_runner.go:164] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"22/tcp\") 0).HostPort}}'\" minikube\n",
      "I0318 14:53:21.263477    2593 ssh_runner.go:195] Run: /bin/bash -c \"sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \\/etc\\/resolv.conf.*/i \\        hosts {\\n           192.168.65.254 host.minikube.internal\\n           fallthrough\\n        }' -e '/^        errors *$/i \\        log' | sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -\"\n",
      "I0318 14:53:21.265464    2593 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml\n",
      "I0318 14:53:21.265472    2593 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)\n",
      "I0318 14:53:21.265549    2593 cli_runner.go:164] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"22/tcp\") 0).HostPort}}'\" minikube\n",
      "I0318 14:53:21.273397    2593 ssh_runner.go:195] Run: sudo systemctl start kubelet\n",
      "I0318 14:53:21.273741    2593 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52320 SSHKeyPath:/Users/kz.li/.minikube/machines/minikube/id_rsa Username:docker}\n",
      "I0318 14:53:21.282471    2593 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52320 SSHKeyPath:/Users/kz.li/.minikube/machines/minikube/id_rsa Username:docker}\n",
      "I0318 14:53:21.381029    2593 start.go:971] {\"host.minikube.internal\": 192.168.65.254} host record injected into CoreDNS's ConfigMap\n",
      "I0318 14:53:21.381211    2593 cli_runner.go:164] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"8443/tcp\") 0).HostPort}}'\" minikube\n",
      "I0318 14:53:21.396992    2593 api_server.go:52] waiting for apiserver process to appear ...\n",
      "I0318 14:53:21.397040    2593 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*\n",
      "I0318 14:53:21.442416    2593 api_server.go:72] duration metric: took 214.28775ms to wait for apiserver process to appear ...\n",
      "I0318 14:53:21.442430    2593 api_server.go:88] waiting for apiserver healthz status ...\n",
      "I0318 14:53:21.442446    2593 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:52324/healthz ...\n",
      "I0318 14:53:21.442749    2593 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml\n",
      "I0318 14:53:21.443430    2593 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml\n",
      "I0318 14:53:21.446041    2593 api_server.go:279] https://127.0.0.1:52324/healthz returned 200:\n",
      "ok\n",
      "I0318 14:53:21.446705    2593 api_server.go:141] control plane version: v1.32.0\n",
      "I0318 14:53:21.446712    2593 api_server.go:131] duration metric: took 4.277958ms to wait for apiserver health ...\n",
      "I0318 14:53:21.446719    2593 system_pods.go:43] waiting for kube-system pods to appear ...\n",
      "I0318 14:53:21.451552    2593 system_pods.go:59] 4 kube-system pods found\n",
      "I0318 14:53:21.451571    2593 system_pods.go:61] \"etcd-minikube\" [f14419b3-50e4-44ab-82cc-5ae61b91957b] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])\n",
      "I0318 14:53:21.451577    2593 system_pods.go:61] \"kube-apiserver-minikube\" [4e53ada7-d51f-4226-8443-c2505057e91f] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])\n",
      "I0318 14:53:21.451585    2593 system_pods.go:61] \"kube-controller-manager-minikube\" [fa86c9f3-73e5-400a-94b2-5c70a20a8021] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])\n",
      "I0318 14:53:21.451589    2593 system_pods.go:61] \"kube-scheduler-minikube\" [4d2bf76f-0c19-44fc-bb67-52056907e7ed] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])\n",
      "I0318 14:53:21.451593    2593 system_pods.go:74] duration metric: took 4.870583ms to wait for pod list to return data ...\n",
      "I0318 14:53:21.451601    2593 kubeadm.go:582] duration metric: took 223.475125ms to wait for: map[apiserver:true system_pods:true]\n",
      "I0318 14:53:21.451610    2593 node_conditions.go:102] verifying NodePressure condition ...\n",
      "I0318 14:53:21.453595    2593 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki\n",
      "I0318 14:53:21.453604    2593 node_conditions.go:123] node cpu capacity is 12\n",
      "I0318 14:53:21.453611    2593 node_conditions.go:105] duration metric: took 1.998459ms to run NodePressure ...\n",
      "I0318 14:53:21.453618    2593 start.go:241] waiting for startup goroutines ...\n",
      "I0318 14:53:21.561712    2593 out.go:177] * Enabled addons: storage-provisioner, default-storageclass\n",
      "I0318 14:53:21.566136    2593 addons.go:514] duration metric: took 338.0435ms for enable addons: enabled=[storage-provisioner default-storageclass]\n",
      "I0318 14:53:21.893058    2593 kapi.go:214] \"coredns\" deployment in \"kube-system\" namespace and \"minikube\" context rescaled to 1 replicas\n",
      "I0318 14:53:21.893100    2593 start.go:246] waiting for cluster config update ...\n",
      "I0318 14:53:21.893128    2593 start.go:255] writing updated cluster config ...\n",
      "I0318 14:53:21.894629    2593 ssh_runner.go:195] Run: rm -f paused\n",
      "I0318 14:53:22.048935    2593 start.go:600] kubectl: 1.32.3, cluster: 1.32.0 (minor skew: 0)\n",
      "I0318 14:53:22.051630    2593 out.go:177] * Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n",
      "\n",
      "\n",
      "[Mar18 06:40] netlink: 'init': attribute type 4 has an invalid length.\n",
      "[  +0.019333] fakeowner: loading out-of-tree module taints kernel.\n",
      "-- No entries --\n",
      "Mar 18 06:53:26 minikube kubelet[2357]: I0318 06:53:26.311498    2357 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"kube-system/storage-provisioner\" podStartSLOduration=5.311486964 podStartE2EDuration=\"5.311486964s\" podCreationTimestamp=\"2025-03-18 06:53:21 +0000 UTC\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-18 06:53:26.311388172 +0000 UTC m=+6.088707129\" watchObservedRunningTime=\"2025-03-18 06:53:26.311486964 +0000 UTC m=+6.088805879\"\n",
      "Mar 18 06:53:26 minikube kubelet[2357]: I0318 06:53:26.311542    2357 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"kube-system/kube-proxy-slcnd\" podStartSLOduration=1.311538422 podStartE2EDuration=\"1.311538422s\" podCreationTimestamp=\"2025-03-18 06:53:25 +0000 UTC\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-18 06:53:26.307837714 +0000 UTC m=+6.085156670\" watchObservedRunningTime=\"2025-03-18 06:53:26.311538422 +0000 UTC m=+6.088857379\"\n",
      "Mar 18 06:53:27 minikube kubelet[2357]: I0318 06:53:27.338184    2357 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"kube-system/coredns-668d6bf9bc-f2hzt\" podStartSLOduration=1.338156298 podStartE2EDuration=\"1.338156298s\" podCreationTimestamp=\"2025-03-18 06:53:26 +0000 UTC\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-18 06:53:27.338024131 +0000 UTC m=+7.115343171\" watchObservedRunningTime=\"2025-03-18 06:53:27.338156298 +0000 UTC m=+7.115475296\"\n",
      "Mar 18 06:53:30 minikube kubelet[2357]: I0318 06:53:30.764543    2357 kuberuntime_manager.go:1702] \"Updating runtime config through cri with podcidr\" CIDR=\"10.244.0.0/24\"\n",
      "Mar 18 06:53:30 minikube kubelet[2357]: I0318 06:53:30.766556    2357 kubelet_network.go:61] \"Updating Pod CIDR\" originalPodCIDR=\"\" newPodCIDR=\"10.244.0.0/24\"\n",
      "Mar 18 06:53:32 minikube kubelet[2357]: I0318 06:53:32.718956    2357 prober_manager.go:312] \"Failed to trigger a manual run\" probe=\"Readiness\"\n",
      "Mar 18 06:53:56 minikube kubelet[2357]: I0318 06:53:56.573718    2357 scope.go:117] \"RemoveContainer\" containerID=\"568769a191af7c938c54938687e33e07464f587dd00d5b501323a4a07d71cde2\"\n",
      "Mar 18 06:55:21 minikube kubelet[2357]: I0318 06:55:21.274599    2357 reconciler_common.go:251] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"podinfo\\\" (UniqueName: \\\"kubernetes.io/downward-api/b60c2594-a90d-4107-952a-ec269a6063d2-podinfo\\\") pod \\\"vault-secrets-operator-controller-manager-79b6cff8b4-nsmms\\\" (UID: \\\"b60c2594-a90d-4107-952a-ec269a6063d2\\\") \" pod=\"vault-secrets-operator-system/vault-secrets-operator-controller-manager-79b6cff8b4-nsmms\"\n",
      "Mar 18 06:55:21 minikube kubelet[2357]: I0318 06:55:21.274669    2357 reconciler_common.go:251] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"kube-api-access-h98d6\\\" (UniqueName: \\\"kubernetes.io/projected/b60c2594-a90d-4107-952a-ec269a6063d2-kube-api-access-h98d6\\\") pod \\\"vault-secrets-operator-controller-manager-79b6cff8b4-nsmms\\\" (UID: \\\"b60c2594-a90d-4107-952a-ec269a6063d2\\\") \" pod=\"vault-secrets-operator-system/vault-secrets-operator-controller-manager-79b6cff8b4-nsmms\"\n",
      "Mar 18 06:55:49 minikube kubelet[2357]: I0318 06:55:49.570648    2357 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"vault-secrets-operator-system/vault-secrets-operator-controller-manager-79b6cff8b4-nsmms\" podStartSLOduration=1.488387281 podStartE2EDuration=\"28.570624877s\" podCreationTimestamp=\"2025-03-18 06:55:21 +0000 UTC\" firstStartedPulling=\"2025-03-18 06:55:21.589077503 +0000 UTC m=+121.367539889\" lastFinishedPulling=\"2025-03-18 06:55:48.67169446 +0000 UTC m=+148.449777485\" observedRunningTime=\"2025-03-18 06:55:49.570391127 +0000 UTC m=+149.348474319\" watchObservedRunningTime=\"2025-03-18 06:55:49.570624877 +0000 UTC m=+149.348708027\"\n",
      ".:53\n",
      "[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169\n",
      "CoreDNS-1.11.3\n",
      "linux/arm64, go1.21.11, a6338e9\n",
      "[INFO] 127.0.0.1:44032 - 51356 \"HINFO IN 8139518595069128429.26510823374384804. udp 55 false 512\" NXDOMAIN qr,rd,ra 55 2.026066126s\n",
      "[INFO] 127.0.0.1:35606 - 7257 \"HINFO IN 8139518595069128429.26510823374384804. udp 55 false 512\" NXDOMAIN qr,rd,ra 55 0.007183166s\n",
      "[INFO] 10.244.0.3:43580 - 6545 \"A IN host.minikube.internal.vault-secrets-operator-system.svc.cluster.local. udp 99 false 1232\" NXDOMAIN qr,aa,rd 181 0.000264041s\n",
      "[INFO] 10.244.0.3:47679 - 40674 \"AAAA IN host.minikube.internal.vault-secrets-operator-system.svc.cluster.local. udp 99 false 1232\" NXDOMAIN qr,aa,rd 181 0.000253833s\n",
      "[INFO] 10.244.0.3:53235 - 62349 \"A IN host.minikube.internal.svc.cluster.local. udp 69 false 1232\" NXDOMAIN qr,aa,rd 151 0.0001065s\n",
      "[INFO] 10.244.0.3:47410 - 39282 \"AAAA IN host.minikube.internal.svc.cluster.local. udp 69 false 1232\" NXDOMAIN qr,aa,rd 151 0.000086959s\n",
      "[INFO] 10.244.0.3:33000 - 55246 \"AAAA IN host.minikube.internal.cluster.local. udp 65 false 1232\" NXDOMAIN qr,aa,rd 147 0.000113584s\n",
      "[INFO] 10.244.0.3:60555 - 33288 \"A IN host.minikube.internal.cluster.local. udp 65 false 1232\" NXDOMAIN qr,aa,rd 147 0.000197875s\n",
      "[INFO] 10.244.0.3:47326 - 27479 \"AAAA IN host.minikube.internal. udp 51 false 1232\" NOERROR qr,aa,rd 40 0.000105709s\n",
      "[INFO] 10.244.0.3:46136 - 43030 \"A IN host.minikube.internal. udp 51 false 1232\" NOERROR qr,aa,rd 78 0.000139042s\n",
      "[INFO] 10.244.0.3:58228 - 49467 \"AAAA IN host.minikube.internal.vault-secrets-operator-system.svc.cluster.local. udp 99 false 1232\" NXDOMAIN qr,aa,rd 181 0.00009825s\n",
      "[INFO] 10.244.0.3:45460 - 55624 \"A IN host.minikube.internal.vault-secrets-operator-system.svc.cluster.local. udp 99 false 1232\" NXDOMAIN qr,aa,rd 181 0.000096125s\n",
      "[INFO] 10.244.0.3:47507 - 36579 \"AAAA IN host.minikube.internal.svc.cluster.local. udp 69 false 1232\" NXDOMAIN qr,aa,rd 151 0.000079958s\n",
      "[INFO] 10.244.0.3:48335 - 12630 \"A IN host.minikube.internal.svc.cluster.local. udp 69 false 1232\" NXDOMAIN qr,aa,rd 151 0.000062875s\n",
      "[INFO] 10.244.0.3:36105 - 59108 \"AAAA IN host.minikube.internal.cluster.local. udp 65 false 1232\" NXDOMAIN qr,aa,rd 147 0.000053042s\n",
      "[INFO] 10.244.0.3:56617 - 24965 \"A IN host.minikube.internal.cluster.local. udp 65 false 1232\" NXDOMAIN qr,aa,rd 147 0.000042084s\n",
      "[INFO] 10.244.0.3:58681 - 7288 \"A IN host.minikube.internal. udp 51 false 1232\" NOERROR qr,aa,rd 78 0.000044s\n",
      "[INFO] 10.244.0.3:47788 - 29839 \"AAAA IN host.minikube.internal. udp 51 false 1232\" NOERROR qr,aa,rd 40 0.000042291s\n",
      "[INFO] 10.244.0.3:36981 - 30093 \"AAAA IN host.minikube.internal.vault-secrets-operator-system.svc.cluster.local. udp 99 false 1232\" NXDOMAIN qr,aa,rd 181 0.000116334s\n",
      "[INFO] 10.244.0.3:54901 - 57392 \"A IN host.minikube.internal.vault-secrets-operator-system.svc.cluster.local. udp 99 false 1232\" NXDOMAIN qr,aa,rd 181 0.000195542s\n",
      "[INFO] 10.244.0.3:45920 - 29630 \"A IN host.minikube.internal.svc.cluster.local. udp 69 false 1232\" NXDOMAIN qr,aa,rd 151 0.000097959s\n",
      "[INFO] 10.244.0.3:44159 - 64042 \"AAAA IN host.minikube.internal.svc.cluster.local. udp 69 false 1232\" NXDOMAIN qr,aa,rd 151 0.0000695s\n",
      "[INFO] 10.244.0.3:48950 - 64301 \"A IN host.minikube.internal.cluster.local. udp 65 false 1232\" NXDOMAIN qr,aa,rd 147 0.000061208s\n",
      "[INFO] 10.244.0.3:59842 - 9915 \"AAAA IN host.minikube.internal.cluster.local. udp 65 false 1232\" NXDOMAIN qr,aa,rd 147 0.000065958s\n",
      "[INFO] 10.244.0.3:45246 - 34659 \"A IN host.minikube.internal. udp 51 false 1232\" NOERROR qr,aa,rd 78 0.000042209s\n",
      "[INFO] 10.244.0.3:36733 - 39255 \"AAAA IN host.minikube.internal. udp 51 false 1232\" NOERROR qr,aa,rd 40 0.000032625s\n",
      "[INFO] 10.244.0.3:38712 - 2951 \"AAAA IN host.minikube.internal.vault-secrets-operator-system.svc.cluster.local. udp 99 false 1232\" NXDOMAIN qr,aa,rd 181 0.000074959s\n",
      "[INFO] 10.244.0.3:59137 - 29135 \"A IN host.minikube.internal.vault-secrets-operator-system.svc.cluster.local. udp 99 false 1232\" NXDOMAIN qr,aa,rd 181 0.000153375s\n",
      "[INFO] 10.244.0.3:50988 - 45472 \"AAAA IN host.minikube.internal.svc.cluster.local. udp 69 false 1232\" NXDOMAIN qr,aa,rd 151 0.000056375s\n",
      "[INFO] 10.244.0.3:42642 - 25599 \"A IN host.minikube.internal.svc.cluster.local. udp 69 false 1232\" NXDOMAIN qr,aa,rd 151 0.000082292s\n",
      "[INFO] 10.244.0.3:37964 - 5796 \"A IN host.minikube.internal.cluster.local. udp 65 false 1232\" NXDOMAIN qr,aa,rd 147 0.000054292s\n",
      "[INFO] 10.244.0.3:56629 - 7391 \"AAAA IN host.minikube.internal.cluster.local. udp 65 false 1232\" NXDOMAIN qr,aa,rd 147 0.000046208s\n",
      "[INFO] 10.244.0.3:44952 - 27233 \"A IN host.minikube.internal. udp 51 false 1232\" NOERROR qr,aa,rd 78 0.000036208s\n",
      "[INFO] 10.244.0.3:35352 - 26964 \"AAAA IN host.minikube.internal. udp 51 false 1232\" NOERROR qr,aa,rd 40 0.000034208s\n",
      "CONTAINER           IMAGE                                                                                                        CREATED             STATE               NAME                      ATTEMPT             POD ID              POD\n",
      "746806bf7dd50       hashicorp/vault-secrets-operator@sha256:291286541d7c3052674021003b907c17e7019b9f499790b0859e1fb8442ffc93     14 minutes ago      Running             manager                   0                   5b14699406f83       vault-secrets-operator-controller-manager-79b6cff8b4-nsmms\n",
      "29b5e9ca7febe       gcr.io/kubebuilder/kube-rbac-proxy@sha256:d8cc6ffb98190e8dd403bfe67ddcb454e6127d32b87acc237b3e5240f70a20fb   14 minutes ago      Running             kube-rbac-proxy           0                   5b14699406f83       vault-secrets-operator-controller-manager-79b6cff8b4-nsmms\n",
      "dee6704514ef3       ba04bb24b9575                                                                                                16 minutes ago      Running             storage-provisioner       1                   477753136ae41       storage-provisioner\n",
      "9c87a2a311f97       2f6c962e7b831                                                                                                16 minutes ago      Running             coredns                   0                   3673d6419b544       coredns-668d6bf9bc-f2hzt\n",
      "313d1d020c6e2       2f50386e20bfd                                                                                                16 minutes ago      Running             kube-proxy                0                   91a74fab7b084       kube-proxy-slcnd\n",
      "568769a191af7       ba04bb24b9575                                                                                                16 minutes ago      Exited              storage-provisioner       0                   477753136ae41       storage-provisioner\n",
      "f6ab4108ca927       2b5bd0f16085a                                                                                                16 minutes ago      Running             kube-apiserver            0                   f6aef990a06a4       kube-apiserver-minikube\n",
      "39d617a06f0dc       a8d049396f6b8                                                                                                16 minutes ago      Running             kube-controller-manager   0                   2605e75f788b0       kube-controller-manager-minikube\n",
      "a33f8efac0e04       7fc9d4aa817aa                                                                                                16 minutes ago      Running             etcd                      0                   c6d73a08709d9       etcd-minikube\n",
      "74dcbae10cd29       c3ff26fb59f37                                                                                                16 minutes ago      Running             kube-scheduler            0                   a19135fd6f4a5       kube-scheduler-minikube\n",
      "Name:               minikube\n",
      "Roles:              control-plane\n",
      "Labels:             beta.kubernetes.io/arch=arm64\n",
      "                    beta.kubernetes.io/os=linux\n",
      "                    kubernetes.io/arch=arm64\n",
      "                    kubernetes.io/hostname=minikube\n",
      "                    kubernetes.io/os=linux\n",
      "                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed\n",
      "                    minikube.k8s.io/name=minikube\n",
      "                    minikube.k8s.io/primary=true\n",
      "                    minikube.k8s.io/updated_at=2025_03_18T14_53_21_0700\n",
      "                    minikube.k8s.io/version=v1.35.0\n",
      "                    node-role.kubernetes.io/control-plane=\n",
      "                    node.kubernetes.io/exclude-from-external-load-balancers=\n",
      "Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock\n",
      "                    node.alpha.kubernetes.io/ttl: 0\n",
      "                    volumes.kubernetes.io/controller-managed-attach-detach: true\n",
      "CreationTimestamp:  Tue, 18 Mar 2025 06:53:18 +0000\n",
      "Taints:             <none>\n",
      "Unschedulable:      false\n",
      "Lease:\n",
      "  HolderIdentity:  minikube\n",
      "  AcquireTime:     <unset>\n",
      "  RenewTime:       Tue, 18 Mar 2025 07:09:59 +0000\n",
      "Conditions:\n",
      "  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n",
      "  ----             ------  -----------------                 ------------------                ------                       -------\n",
      "  MemoryPressure   False   Tue, 18 Mar 2025 07:09:08 +0000   Tue, 18 Mar 2025 06:53:18 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n",
      "  DiskPressure     False   Tue, 18 Mar 2025 07:09:08 +0000   Tue, 18 Mar 2025 06:53:18 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n",
      "  PIDPressure      False   Tue, 18 Mar 2025 07:09:08 +0000   Tue, 18 Mar 2025 06:53:18 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n",
      "  Ready            True    Tue, 18 Mar 2025 07:09:08 +0000   Tue, 18 Mar 2025 06:53:18 +0000   KubeletReady                 kubelet is posting ready status\n",
      "Addresses:\n",
      "  InternalIP:  192.168.49.2\n",
      "  Hostname:    minikube\n",
      "Capacity:\n",
      "  cpu:                12\n",
      "  ephemeral-storage:  61202244Ki\n",
      "  hugepages-1Gi:      0\n",
      "  hugepages-2Mi:      0\n",
      "  hugepages-32Mi:     0\n",
      "  hugepages-64Ki:     0\n",
      "  memory:             8025084Ki\n",
      "  pods:               110\n",
      "Allocatable:\n",
      "  cpu:                12\n",
      "  ephemeral-storage:  61202244Ki\n",
      "  hugepages-1Gi:      0\n",
      "  hugepages-2Mi:      0\n",
      "  hugepages-32Mi:     0\n",
      "  hugepages-64Ki:     0\n",
      "  memory:             8025084Ki\n",
      "  pods:               110\n",
      "System Info:\n",
      "  Machine ID:                    09c16876f1c44c39b64e6d29e01c1c00\n",
      "  System UUID:                   09c16876f1c44c39b64e6d29e01c1c00\n",
      "  Boot ID:                       47a44d93-5e46-4408-8c37-98f04ec31ddc\n",
      "  Kernel Version:                6.10.14-linuxkit\n",
      "  OS Image:                      Ubuntu 22.04.5 LTS\n",
      "  Operating System:              linux\n",
      "  Architecture:                  arm64\n",
      "  Container Runtime Version:     docker://27.4.1\n",
      "  Kubelet Version:               v1.32.0\n",
      "  Kube-Proxy Version:            v1.32.0\n",
      "PodCIDR:                         10.244.0.0/24\n",
      "PodCIDRs:                        10.244.0.0/24\n",
      "Non-terminated Pods:             (8 in total)\n",
      "  Namespace                      Name                                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n",
      "  ---------                      ----                                                          ------------  ----------  ---------------  -------------  ---\n",
      "  kube-system                    coredns-668d6bf9bc-f2hzt                                      100m (0%)     0 (0%)      70Mi (0%)        170Mi (2%)     16m\n",
      "  kube-system                    etcd-minikube                                                 100m (0%)     0 (0%)      100Mi (1%)       0 (0%)         16m\n",
      "  kube-system                    kube-apiserver-minikube                                       250m (2%)     0 (0%)      0 (0%)           0 (0%)         16m\n",
      "  kube-system                    kube-controller-manager-minikube                              200m (1%)     0 (0%)      0 (0%)           0 (0%)         16m\n",
      "  kube-system                    kube-proxy-slcnd                                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         16m\n",
      "  kube-system                    kube-scheduler-minikube                                       100m (0%)     0 (0%)      0 (0%)           0 (0%)         16m\n",
      "  kube-system                    storage-provisioner                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         16m\n",
      "  vault-secrets-operator-system  vault-secrets-operator-controller-manager-79b6cff8b4-nsmms    15m (0%)      1 (8%)      128Mi (1%)       256Mi (3%)     14m\n",
      "Allocated resources:\n",
      "  (Total limits may be over 100 percent, i.e., overcommitted.)\n",
      "  Resource           Requests    Limits\n",
      "  --------           --------    ------\n",
      "  cpu                765m (6%)   1 (8%)\n",
      "  memory             298Mi (3%)  426Mi (5%)\n",
      "  ephemeral-storage  0 (0%)      0 (0%)\n",
      "  hugepages-1Gi      0 (0%)      0 (0%)\n",
      "  hugepages-2Mi      0 (0%)      0 (0%)\n",
      "  hugepages-32Mi     0 (0%)      0 (0%)\n",
      "  hugepages-64Ki     0 (0%)      0 (0%)\n",
      "Events:\n",
      "  Type    Reason                   Age   From             Message\n",
      "  ----    ------                   ----  ----             -------\n",
      "  Normal  Starting                 16m   kube-proxy       \n",
      "  Normal  Starting                 16m   kubelet          Starting kubelet.\n",
      "  Normal  NodeAllocatableEnforced  16m   kubelet          Updated Node Allocatable limit across pods\n",
      "  Normal  NodeHasSufficientMemory  16m   kubelet          Node minikube status is now: NodeHasSufficientMemory\n",
      "  Normal  NodeHasNoDiskPressure    16m   kubelet          Node minikube status is now: NodeHasNoDiskPressure\n",
      "  Normal  NodeHasSufficientPID     16m   kubelet          Node minikube status is now: NodeHasSufficientPID\n",
      "  Normal  RegisteredNode           16m   node-controller  Node minikube event: Registered Node minikube in Controller\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Follow minikube logs\n",
    "minikube logs -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
